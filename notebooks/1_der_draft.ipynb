{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/esthy13/cil-intrusion-detection/blob/main/der_draft.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NkCuth4CTgIB",
      "metadata": {
        "id": "NkCuth4CTgIB"
      },
      "source": [
        "# DER++ for Intrusion Detection (CIC-IDS)\n",
        "\n",
        "Minimal **working** implementation of **Dark Experience Replay++**\n",
        "for class-incremental intrusion detection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "383d2c74",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "383d2c74",
        "outputId": "2a8870ca-f6fa-4e0a-9282-651909ddc447"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'cil-intrusion-detection' already exists and is not an empty directory.\n",
            "/content/cil-intrusion-detection\n",
            "Already up to date.\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/esthy13/cil-intrusion-detection\n",
        "%cd cil-intrusion-detection\n",
        "!git pull\n",
        "# resetting the path to content to avoid issues in the rest of the notebook\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "EOVxYZQ2TgID",
      "metadata": {
        "id": "EOVxYZQ2TgID"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RDA-qUhCTgID",
      "metadata": {
        "id": "RDA-qUhCTgID"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "nGoN76koTgID",
      "metadata": {
        "id": "nGoN76koTgID"
      },
      "outputs": [],
      "source": [
        "class IDSBaseDataset(Dataset):\n",
        "    def __init__(self, root_dir, split=\"train\"):\n",
        "        \"\"\"\n",
        "        root_dir: path to 2017/\n",
        "        split: 'train' or 'test'\n",
        "        \"\"\"\n",
        "        csv_dir = os.path.join(root_dir, split)\n",
        "        csvs = glob.glob(os.path.join(csv_dir, \"*.csv\"))\n",
        "        assert len(csvs) > 0, f\"No CSV files found in {csv_dir}\"\n",
        "\n",
        "        df = pd.concat([pd.read_csv(c) for c in csvs], ignore_index=True)\n",
        "\n",
        "        self.classes = sorted(df[\"Label\"].unique())\n",
        "        self.class_to_idx = {c: i for i, c in enumerate(self.classes)}\n",
        "\n",
        "        self.x = df.drop(columns=[\"Label\"]).values.astype(np.float32)\n",
        "        self.y = np.array(\n",
        "            [self.class_to_idx[label] for label in df[\"Label\"]],\n",
        "            dtype=np.int64\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.x[idx]), torch.tensor(self.y[idx])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-d0BVvGMTgID",
      "metadata": {
        "id": "-d0BVvGMTgID"
      },
      "source": [
        "## Task builder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "dc-LvNYbWDtQ",
      "metadata": {
        "id": "dc-LvNYbWDtQ"
      },
      "outputs": [],
      "source": [
        "class RemappedSubset(Dataset):\n",
        "    \"\"\"\n",
        "    Subset that remaps global class indices to [0..C-1]\n",
        "    \"\"\"\n",
        "    def __init__(self, dataset, indices, class_ids):\n",
        "        self.dataset = dataset\n",
        "        self.indices = indices\n",
        "        self.class_map = {cid: i for i, cid in enumerate(class_ids)}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x, y = self.dataset[self.indices[idx]]\n",
        "        return x, torch.tensor(self.class_map[y.item()])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "a58aa056",
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_task(dataset, class_names):\n",
        "    class_ids = [dataset.class_to_idx[c] for c in class_names]\n",
        "    idxs = np.where(np.isin(dataset.y, class_ids))[0]\n",
        "    return RemappedSubset(dataset, idxs, class_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "dKlk2M2WTgID",
      "metadata": {
        "id": "dKlk2M2WTgID"
      },
      "outputs": [],
      "source": [
        "def build_scenario(all_classes, splits):\n",
        "    \"\"\"\n",
        "    all_classes: list of class names\n",
        "    splits: list of ints, e.g. [1,1,1,1] or [5,5] or [2,3,5]\n",
        "\n",
        "    Returns: list of task class lists\n",
        "    \"\"\"\n",
        "    print(f\"--- Scenario with splits: {splits} ---\")\n",
        "    tasks = []\n",
        "    idx = 0\n",
        "    for s in splits:\n",
        "        tasks.append(all_classes[idx:idx+s])\n",
        "        idx += s\n",
        "    assert idx == len(all_classes), \"Splits do not sum to total classes\"\n",
        "    return tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gyLmpf46TgID",
      "metadata": {
        "id": "gyLmpf46TgID"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "MD8KacN6TgID",
      "metadata": {
        "id": "MD8KacN6TgID"
      },
      "outputs": [],
      "source": [
        "class CILModel(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes):\n",
        "        super().__init__()\n",
        "        self.fe = nn.Sequential(\n",
        "            nn.Linear(input_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 256),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.classifier = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.fe(x)\n",
        "        return self.classifier(z)\n",
        "\n",
        "    def expand_classes(self, n_new):\n",
        "        old = self.classifier\n",
        "        new = nn.Linear(old.in_features, old.out_features + n_new).to(device)\n",
        "        new.weight.data[:old.out_features] = old.weight.data\n",
        "        new.bias.data[:old.out_features] = old.bias.data\n",
        "        self.classifier = new"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fzKe9gvYTgID",
      "metadata": {
        "id": "fzKe9gvYTgID"
      },
      "source": [
        "## Replay Buffer (DER++)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "QV41fmpmTgIE",
      "metadata": {
        "id": "QV41fmpmTgIE"
      },
      "outputs": [],
      "source": [
        "class ReservoirBuffer:\n",
        "    def __init__(self, size):\n",
        "        self.size = size\n",
        "        self.n_seen = 0\n",
        "        self.x, self.y, self.logits = [], [], []\n",
        "\n",
        "    def add(self, x, y, logits):\n",
        "        for xi, yi, li in zip(x, y, logits):\n",
        "            li = li.detach().cpu()\n",
        "            if len(self.x) < self.size:\n",
        "                self.x.append(xi.cpu())\n",
        "                self.y.append(yi.cpu())\n",
        "                self.logits.append(li)\n",
        "            else:\n",
        "                j = np.random.randint(0, self.n_seen + 1)\n",
        "                if j < self.size:\n",
        "                    self.x[j] = xi.cpu()\n",
        "                    self.y[j] = yi.cpu()\n",
        "                    self.logits[j] = li\n",
        "            self.n_seen += 1\n",
        "\n",
        "    def sample(self, batch_size, current_dim):\n",
        "        if len(self.x) == 0:\n",
        "            return None\n",
        "\n",
        "        idx = np.random.choice(len(self.x), min(batch_size, len(self.x)), replace=False)\n",
        "\n",
        "        bx = torch.stack([self.x[i] for i in idx]).to(device)\n",
        "        by = torch.stack([self.y[i] for i in idx]).to(device)\n",
        "\n",
        "        # PAD LOGITS\n",
        "        padded_logits = []\n",
        "        for i in idx:\n",
        "            old_logit = self.logits[i]\n",
        "            if old_logit.numel() < current_dim:\n",
        "                pad = torch.zeros(current_dim - old_logit.numel())\n",
        "                old_logit = torch.cat([old_logit, pad])\n",
        "            padded_logits.append(old_logit)\n",
        "\n",
        "        blogits = torch.stack(padded_logits).to(device)\n",
        "\n",
        "        return bx, by, blogits"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HUKM5dMBTgIE",
      "metadata": {
        "id": "HUKM5dMBTgIE"
      },
      "source": [
        "## DER++ Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "Q0yY9tpjTgIE",
      "metadata": {
        "id": "Q0yY9tpjTgIE"
      },
      "outputs": [],
      "source": [
        "def train_task(model, loader, buffer, optimizer, alpha=0.5, beta=0.5, epochs=1):\n",
        "    ce = nn.CrossEntropyLoss()\n",
        "\n",
        "    model.train()\n",
        "    for _ in range(epochs):\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "\n",
        "            out = model(x)\n",
        "            loss = ce(out, y)\n",
        "\n",
        "            buf = buffer.sample(len(x), model.classifier.out_features)\n",
        "            if buf:\n",
        "                bx, by, blog = buf\n",
        "                loss += alpha * F.mse_loss(model(bx), blog)\n",
        "                loss += beta * ce(model(bx), by)\n",
        "\n",
        "\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            buffer.add(x, y, out.detach())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "63021c53",
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate(model, dataset, classes, scenario):\n",
        "    model.eval()\n",
        "    loader = DataLoader(build_scenario(classes, scenario), batch_size=256)\n",
        "    print(loader.dataset.class_map)\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            preds = model(x).argmax(1)\n",
        "            correct += (preds == y).sum().item()\n",
        "            total += y.size(0)\n",
        "    return correct / total if total > 0 else 0"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gmG72ZZQTgIE",
      "metadata": {
        "id": "gmG72ZZQTgIE"
      },
      "source": [
        "## Run Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "cc5f7872",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cc5f7872",
        "outputId": "f4c26f96-d027-4928-fbe1-d691a8708a6a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  cil-intrusion-detection/data/processed/2017.zip\n",
            "replace 2017/train/portscan.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "!unzip cil-intrusion-detection/data/processed/2017.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wU852nUOTgIE",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wU852nUOTgIE",
        "outputId": "8fcb476a-bc27-49cd-b1c3-4d6d4e26347a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'benign': 0, 'bot': 1, 'ddos': 2, 'dos': 3, 'ftp-patator': 4, 'portscan': 5, 'ssh-patator': 6, 'web-attack': 7}\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Paths\n",
        "DATA_ROOT = \"2017\"  # <-- folder created by unzip\n",
        "\n",
        "# Datasets\n",
        "train_dataset = IDSBaseDataset(DATA_ROOT, split=\"train\")\n",
        "test_dataset  = IDSBaseDataset(DATA_ROOT, split=\"test\")\n",
        "\n",
        "print(train_dataset.class_to_idx)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7ae59f6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Scenario: [['benign'], ['dos'], ['ddos'], ['portscan'], ['ssh-patator'], ['ftp-patator'], ['web-attack'], ['bot']] ===\n",
            "\n",
            "=== Task 0: ['benign']\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3169983509.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m         )\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         train_task(\n\u001b[0m\u001b[1;32m     53\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1991852175.py\u001b[0m in \u001b[0;36mtrain_task\u001b[0;34m(model, loader, buffer, optimizer, alpha, beta, epochs)\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-1285004706.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, x, y, logits)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mxi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mli\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0mli\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mli\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "input_dim = train_dataset.x.shape[1]\n",
        "\n",
        "# Task definition (example)\n",
        "all_classes = [\n",
        "    \"benign\",\n",
        "    \"dos\",\n",
        "    \"ddos\",\n",
        "    \"portscan\",\n",
        "    \"ssh-patator\",\n",
        "    \"ftp-patator\",\n",
        "    \"web-attack\",\n",
        "    \"bot\"\n",
        "]\n",
        "\n",
        "# Scenario A: 1+1+1+1+1+1+1+1\n",
        "tasks_1 = build_scenario(all_classes, [1]*8)\n",
        "\n",
        "# Scenario B: 5+3\n",
        "tasks_2 = build_scenario(all_classes, [5, 3])\n",
        "\n",
        "# Scenario C: 2+3+3\n",
        "tasks_3 = build_scenario(all_classes, [2, 3, 3])\n",
        "\n",
        "for task in [tasks_1, tasks_2, tasks_3]:\n",
        "    # Model + buffer\n",
        "    model = CILModel(input_dim, len(task[0])).to(device)\n",
        "    buffer = ReservoirBuffer(size=2000)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "    seen_classes = []\n",
        "\n",
        "\n",
        "\n",
        "    # training loop that works only for scenario 1+1+1+1+1...\n",
        "    # TODO create a build scenario method that works for any class split\n",
        "    for task_id, task_classes in enumerate(task):\n",
        "        print(f\"\\n=== Task {task_id}: {task_classes}\")\n",
        "\n",
        "        if task_id > 0:\n",
        "            model.expand_classes(len(task_classes))\n",
        "            optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "        seen_classes += task_classes\n",
        "\n",
        "        train_loader = DataLoader(\n",
        "            build_task(train_dataset, seen_classes),\n",
        "            batch_size=128,\n",
        "            shuffle=True\n",
        "        )\n",
        "\n",
        "        train_task(\n",
        "            model,\n",
        "            train_loader,\n",
        "            buffer,\n",
        "            optimizer,\n",
        "            alpha=0.5,\n",
        "            beta=0.5,\n",
        "            epochs=1\n",
        "        )\n",
        "\n",
        "        acc = evaluate(model, test_dataset, seen_classes)\n",
        "        print(f\"Test accuracy (seen classes): {acc:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
