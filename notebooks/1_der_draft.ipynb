{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/esthy13/cil-intrusion-detection/blob/main/notebooks/1_der_draft.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NkCuth4CTgIB",
      "metadata": {
        "id": "NkCuth4CTgIB"
      },
      "source": [
        "# DER++ for Intrusion Detection (CIC-IDS)\n",
        "\n",
        "Minimal **working** implementation of **Dark Experience Replay++**\n",
        "for class-incremental intrusion detection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "383d2c74",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "383d2c74",
        "outputId": "6b6f85a4-3da3-445b-e2e4-0305315c2f78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'cil-intrusion-detection' already exists and is not an empty directory.\n",
            "/content/cil-intrusion-detection\n",
            "Already up to date.\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/esthy13/cil-intrusion-detection\n",
        "%cd cil-intrusion-detection\n",
        "!git pull\n",
        "# resetting the path to content to avoid issues in the rest of the notebook\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "EOVxYZQ2TgID",
      "metadata": {
        "id": "EOVxYZQ2TgID"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RDA-qUhCTgID",
      "metadata": {
        "id": "RDA-qUhCTgID"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "nGoN76koTgID",
      "metadata": {
        "id": "nGoN76koTgID"
      },
      "outputs": [],
      "source": [
        "class IDSBaseDataset(Dataset):\n",
        "    def __init__(self, root_dir, split=\"train\"):\n",
        "        \"\"\"\n",
        "        root_dir: path to 2017/\n",
        "        split: 'train' or 'test'\n",
        "        \"\"\"\n",
        "        csv_dir = os.path.join(root_dir, split)\n",
        "        csvs = glob.glob(os.path.join(csv_dir, \"*.csv\"))\n",
        "        assert len(csvs) > 0, f\"No CSV files found in {csv_dir}\"\n",
        "\n",
        "        df = pd.concat([pd.read_csv(c) for c in csvs], ignore_index=True)\n",
        "\n",
        "        labels = list(df[\"Label\"].unique())\n",
        "\n",
        "        if \"benign\" not in labels:\n",
        "            raise ValueError(\"Dataset must contain a 'benign' class\")\n",
        "\n",
        "        # Enforcing benign as class 0\n",
        "        labels = [\"benign\"] + sorted([l for l in labels if l != \"benign\"])\n",
        "\n",
        "        self.classes = labels\n",
        "        self.class_to_idx = {c: i for i, c in enumerate(self.classes)}\n",
        "\n",
        "        self.x = df.drop(columns=[\"Label\"]).values.astype(np.float32)\n",
        "        self.y = np.array(\n",
        "            [self.class_to_idx[label] for label in df[\"Label\"]],\n",
        "            dtype=np.int64\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.x[idx]), torch.tensor(self.y[idx])\n",
        "    def set_features(self, new_x):\n",
        "        assert new_x.shape == self.x.shape\n",
        "        self.x = new_x.astype(np.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-d0BVvGMTgID",
      "metadata": {
        "id": "-d0BVvGMTgID"
      },
      "source": [
        "## Task builder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "dc-LvNYbWDtQ",
      "metadata": {
        "id": "dc-LvNYbWDtQ"
      },
      "outputs": [],
      "source": [
        "class RemappedSubset(Dataset):\n",
        "    \"\"\"\n",
        "    Subset that remaps global class indices to [0..C-1]\n",
        "    \"\"\"\n",
        "    def __init__(self, dataset, indices, class_ids):\n",
        "        self.dataset = dataset\n",
        "        self.indices = indices\n",
        "        self.class_map = {cid: i for i, cid in enumerate(class_ids)}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x, y = self.dataset[self.indices[idx]]\n",
        "        return x, torch.tensor(self.class_map[y.item()])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "a58aa056",
      "metadata": {
        "id": "a58aa056"
      },
      "outputs": [],
      "source": [
        "def build_task(dataset, class_names):\n",
        "    class_ids = [dataset.class_to_idx[c] for c in class_names]\n",
        "    idxs = np.where(np.isin(dataset.y, class_ids))[0]\n",
        "    return RemappedSubset(dataset, idxs, class_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "dKlk2M2WTgID",
      "metadata": {
        "id": "dKlk2M2WTgID"
      },
      "outputs": [],
      "source": [
        "def build_scenario( all_classes, attacks_pattern, benign_class=\"benign\"):\n",
        "    \"\"\"\n",
        "    all_classes: ordered list of class names (benign must be first)\n",
        "    attacks_pattern: list of ints, number of NEW attacks per task\n",
        "                     e.g. [1,1,1] or [3,2] or [5]\n",
        "    benign_class: name of benign class (default: 'benign')\n",
        "\n",
        "    Returns:\n",
        "        tasks: list of lists of class names (cumulative)\n",
        "    \"\"\"\n",
        "\n",
        "    if benign_class not in all_classes:\n",
        "        raise ValueError(f\"Benign class '{benign_class}' not found in classes\")\n",
        "\n",
        "    if all_classes[0] != benign_class:\n",
        "        raise ValueError(\n",
        "            f\"Benign class must be index 0, got {all_classes[0]}\"\n",
        "        )\n",
        "\n",
        "    attack_classes = [c for c in all_classes if c != benign_class]\n",
        "\n",
        "    if sum(attacks_pattern) != len(attack_classes):\n",
        "        raise ValueError(\n",
        "            f\"Invalid attacks_pattern: sum={sum(attacks_pattern)}, \"\n",
        "            f\"but there are {len(attack_classes)} attack classes\"\n",
        "        )\n",
        "\n",
        "    tasks = []\n",
        "    current_index = 0\n",
        "\n",
        "    for _, n_new in enumerate(attacks_pattern):\n",
        "        current_index += n_new\n",
        "        seen_attacks = attack_classes[:current_index]\n",
        "        seen_classes = [benign_class] + seen_attacks\n",
        "        tasks.append(seen_classes)\n",
        "\n",
        "    return tasks\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class UpToNormalizer:\n",
        "    \"\"\"\n",
        "    Continual min-max normalizer using only past and present data\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.min = None\n",
        "        self.max = None\n",
        "\n",
        "    def update(self, x):\n",
        "        \"\"\"\n",
        "        x: numpy array [N, D]\n",
        "        \"\"\"\n",
        "        batch_min = x.min(axis=0)\n",
        "        batch_max = x.max(axis=0)\n",
        "\n",
        "        if self.min is None:\n",
        "            self.min = batch_min\n",
        "            self.max = batch_max\n",
        "        else:\n",
        "            self.min = np.minimum(self.min, batch_min)\n",
        "            self.max = np.maximum(self.max, batch_max)\n",
        "\n",
        "    def normalize(self, x):\n",
        "        \"\"\"\n",
        "        x: numpy array [N, D]\n",
        "        \"\"\"\n",
        "        eps = 1e-8\n",
        "        return (x - self.min) / (self.max - self.min + eps)"
      ],
      "metadata": {
        "id": "Wb3m_XX5PM08"
      },
      "id": "Wb3m_XX5PM08",
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "gyLmpf46TgID",
      "metadata": {
        "id": "gyLmpf46TgID"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "MD8KacN6TgID",
      "metadata": {
        "id": "MD8KacN6TgID"
      },
      "outputs": [],
      "source": [
        "class CILModel(nn.Module):\n",
        "    def __init__(self, input_dim, feature_dim=128):\n",
        "        super().__init__()\n",
        "\n",
        "        self.feature_extractor = nn.Sequential(\n",
        "            nn.Linear(input_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, feature_dim)\n",
        "        )\n",
        "\n",
        "        self.classifier = None  # SOLO para entrenamiento\n",
        "\n",
        "    def forward(self, x):\n",
        "        feats = self.feature_extractor(x)\n",
        "        feats = F.normalize(feats, dim=1)\n",
        "\n",
        "        if self.classifier is not None:\n",
        "            logits = self.classifier(feats)\n",
        "            return logits, feats\n",
        "\n",
        "        return feats\n",
        "\n",
        "    def update_classifier(self, num_classes):\n",
        "        self.classifier = nn.Linear(\n",
        "            self.feature_extractor[-1].out_features,\n",
        "            num_classes\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fzKe9gvYTgID",
      "metadata": {
        "id": "fzKe9gvYTgID"
      },
      "source": [
        "## Replay Buffer (DER++)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "QV41fmpmTgIE",
      "metadata": {
        "id": "QV41fmpmTgIE"
      },
      "outputs": [],
      "source": [
        "class ReservoirBuffer:\n",
        "    def __init__(self, size):\n",
        "        self.size = size\n",
        "        self.indices = []\n",
        "        self.labels = []\n",
        "        self.logits = []\n",
        "        self.n_seen = 0\n",
        "\n",
        "    def add(self, indices, labels, logits):\n",
        "        \"\"\"\n",
        "        indices: list[int]\n",
        "        labels: tensor [B]\n",
        "        logits: tensor [B, C]\n",
        "        \"\"\"\n",
        "        indices = indices.tolist() if torch.is_tensor(indices) else indices\n",
        "        labels = labels.detach().cpu()\n",
        "        logits = logits.detach().cpu()\n",
        "\n",
        "        for idx, y, logit in zip(indices, labels, logits):\n",
        "            self.n_seen += 1\n",
        "\n",
        "            if len(self.indices) < self.size:\n",
        "                self.indices.append(idx)\n",
        "                self.labels.append(y)\n",
        "                self.logits.append(logit)\n",
        "            else:\n",
        "                j = random.randint(0, self.n_seen - 1)\n",
        "                if j < self.size:\n",
        "                    self.indices[j] = idx\n",
        "                    self.labels[j] = y\n",
        "                    self.logits[j] = logit\n",
        "\n",
        "    def sample(self, batch_size, current_n_classes):\n",
        "        if len(self.indices) == 0:\n",
        "            return None\n",
        "\n",
        "        idxs = np.random.choice(\n",
        "            len(self.indices),\n",
        "            min(batch_size, len(self.indices)),\n",
        "            replace=False\n",
        "        )\n",
        "\n",
        "        indices = [self.indices[i] for i in idxs]\n",
        "        labels = torch.stack([self.labels[i] for i in idxs])\n",
        "\n",
        "        padded_logits = []\n",
        "        for i in idxs:\n",
        "            logit = self.logits[i]\n",
        "\n",
        "            if logit.shape[0] < current_n_classes:\n",
        "                pad = torch.zeros(\n",
        "                    current_n_classes - logit.shape[0]\n",
        "                )\n",
        "                logit = torch.cat([logit, pad], dim=0)\n",
        "\n",
        "            padded_logits.append(logit)\n",
        "\n",
        "        logits = torch.stack(padded_logits)\n",
        "\n",
        "        return indices, labels, logits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HUKM5dMBTgIE",
      "metadata": {
        "id": "HUKM5dMBTgIE"
      },
      "source": [
        "## DER++ Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "Q0yY9tpjTgIE",
      "metadata": {
        "id": "Q0yY9tpjTgIE"
      },
      "outputs": [],
      "source": [
        "def train_task(model, loader, buffer, optimizer,\n",
        "               alpha=0.5, beta=0.5, epochs=1):\n",
        "\n",
        "    ce = nn.CrossEntropyLoss()\n",
        "    model.train()\n",
        "\n",
        "    for _ in range(epochs):\n",
        "        for batch_idx, (x, y) in enumerate(loader):\n",
        "\n",
        "            x, y = x.to(device), y.to(device)\n",
        "\n",
        "            # Forward current batch\n",
        "            logits, _ = model(x)\n",
        "            loss = ce(logits, y)\n",
        "\n",
        "            # ----- DER++ Replay -----\n",
        "            buf = buffer.sample(len(x), model.classifier.out_features)\n",
        "\n",
        "            if buf is not None:\n",
        "                replay_indices, replay_labels, replay_logits = buf\n",
        "\n",
        "                # Re-fetch from BASE dataset (global indices)\n",
        "                bx = torch.stack([\n",
        "                    loader.dataset.dataset[i][0]\n",
        "                    for i in replay_indices\n",
        "                ]).to(device)\n",
        "\n",
        "                by = replay_labels.to(device)\n",
        "                blog = replay_logits.to(device)\n",
        "\n",
        "                replay_out, _ = model(bx)\n",
        "\n",
        "                # Expand stored logits if classifier grew\n",
        "                if blog.shape[1] < replay_out.shape[1]:\n",
        "                    pad = torch.zeros(\n",
        "                        blog.shape[0],\n",
        "                        replay_out.shape[1] - blog.shape[1],\n",
        "                        device=device\n",
        "                    )\n",
        "                    blog = torch.cat([blog, pad], dim=1)\n",
        "\n",
        "                # DER++\n",
        "                loss += alpha * F.mse_loss(replay_out, blog)\n",
        "                loss += beta * ce(replay_out, by)\n",
        "\n",
        "            # Backprop\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # ----- Add current batch to buffer -----\n",
        "            original_indices = [\n",
        "                loader.dataset.indices[i]\n",
        "                for i in range(\n",
        "                    batch_idx * loader.batch_size,\n",
        "                    batch_idx * loader.batch_size + len(x)\n",
        "                )\n",
        "            ]\n",
        "\n",
        "            buffer.add(original_indices, y, logits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "63021c53",
      "metadata": {
        "id": "63021c53"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, dataset, seen_classes):\n",
        "    model.eval()\n",
        "\n",
        "    eval_dataset = build_task(dataset, seen_classes)\n",
        "    loader = DataLoader(eval_dataset, batch_size=256, shuffle=False)\n",
        "\n",
        "    all_preds, all_targets = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x = x.to(device)\n",
        "            logits, _ = model(x)\n",
        "            preds = logits.argmax(1).cpu().numpy()\n",
        "\n",
        "            all_preds.append(preds)\n",
        "            all_targets.append(y.numpy())\n",
        "\n",
        "    all_preds = np.concatenate(all_preds)\n",
        "    all_targets = np.concatenate(all_targets)\n",
        "\n",
        "    acc = accuracy_score(all_targets, all_preds)\n",
        "    f1  = f1_score(all_targets, all_preds, average=\"macro\")\n",
        "\n",
        "    return acc, f1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gmG72ZZQTgIE",
      "metadata": {
        "id": "gmG72ZZQTgIE"
      },
      "source": [
        "## Run Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "cc5f7872",
      "metadata": {
        "id": "cc5f7872"
      },
      "outputs": [],
      "source": [
        "# !unzip cil-intrusion-detection/data/processed/2017.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "wU852nUOTgIE",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wU852nUOTgIE",
        "outputId": "9fba88db-e470-4fd5-c339-240f56491ec4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'benign': 0, 'bot': 1, 'ddos': 2, 'dos': 3, 'ftp-patator': 4, 'portscan': 5, 'ssh-patator': 6, 'web-attack': 7}\n"
          ]
        }
      ],
      "source": [
        "# Paths\n",
        "DATA_ROOT = \"2017\"  # <-- folder created by unzip\n",
        "\n",
        "# Datasets\n",
        "train_dataset = IDSBaseDataset(DATA_ROOT, split=\"train\")\n",
        "test_dataset  = IDSBaseDataset(DATA_ROOT, split=\"test\")\n",
        "\n",
        "print(train_dataset.class_to_idx)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "f7ae59f6",
      "metadata": {
        "id": "f7ae59f6",
        "outputId": "fa78ab06-1dde-46d7-8e64-2b10df5908dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Scenario 1 ===\n",
            "\n",
            "=== Task 0: ['benign', 'dos']\n",
            "Accuracy: 0.9565 | Macro-F1: 0.8486\n",
            "\n",
            "=== Task 1: ['benign', 'dos', 'ddos']\n",
            "Accuracy: 0.9737 | Macro-F1: 0.9339\n",
            "\n",
            "=== Task 2: ['benign', 'dos', 'ddos', 'portscan']\n",
            "Accuracy: 0.9206 | Macro-F1: 0.8157\n",
            "\n",
            "=== Task 3: ['benign', 'dos', 'ddos', 'portscan', 'ssh-patator']\n",
            "Accuracy: 0.9323 | Macro-F1: 0.6852\n",
            "\n",
            "=== Task 4: ['benign', 'dos', 'ddos', 'portscan', 'ssh-patator', 'ftp-patator']\n",
            "Accuracy: 0.9305 | Macro-F1: 0.7826\n",
            "\n",
            "=== Task 5: ['benign', 'dos', 'ddos', 'portscan', 'ssh-patator', 'ftp-patator', 'web-attack']\n",
            "Accuracy: 0.9510 | Macro-F1: 0.7065\n",
            "\n",
            "=== Task 6: ['benign', 'dos', 'ddos', 'portscan', 'ssh-patator', 'ftp-patator', 'web-attack', 'bot']\n",
            "Accuracy: 0.9541 | Macro-F1: 0.4548\n",
            "\n",
            "=== Scenario 2 ===\n",
            "\n",
            "=== Task 0: ['benign', 'dos', 'ddos', 'portscan']\n",
            "Accuracy: 0.8083 | Macro-F1: 0.2235\n",
            "\n",
            "=== Task 1: ['benign', 'dos', 'ddos', 'portscan', 'ssh-patator', 'ftp-patator', 'web-attack', 'bot']\n",
            "Accuracy: 0.9469 | Macro-F1: 0.4378\n",
            "\n",
            "=== Scenario 3 ===\n",
            "\n",
            "=== Task 0: ['benign', 'dos']\n",
            "Accuracy: 0.9562 | Macro-F1: 0.8478\n",
            "\n",
            "=== Task 1: ['benign', 'dos', 'ddos', 'portscan', 'ssh-patator']\n",
            "Accuracy: 0.9541 | Macro-F1: 0.7230\n",
            "\n",
            "=== Task 2: ['benign', 'dos', 'ddos', 'portscan', 'ssh-patator', 'ftp-patator', 'web-attack', 'bot']\n",
            "Accuracy: 0.9553 | Macro-F1: 0.5187\n"
          ]
        }
      ],
      "source": [
        "input_dim = train_dataset.x.shape[1]\n",
        "\n",
        "# Task definition (example)\n",
        "all_classes = [\n",
        "    \"benign\",\n",
        "    \"dos\",\n",
        "    \"ddos\",\n",
        "    \"portscan\",\n",
        "    \"ssh-patator\",\n",
        "    \"ftp-patator\",\n",
        "    \"web-attack\",\n",
        "    \"bot\"\n",
        "]\n",
        "\n",
        "scenarios = []\n",
        "\n",
        "# # Scenario A: 1+1+1+1+1+1+1+1\n",
        "# scenarios.append(build_scenario(all_classes, [1,1,1,1,1,1,1]))\n",
        "\n",
        "# Scenario B: 5+3\n",
        "scenarios.append(build_scenario(all_classes, [3, 4]))\n",
        "\n",
        "# # Scenario C: 2+3+3\n",
        "# scenarios.append(build_scenario(all_classes, [1, 3, 3]))\n",
        "\n",
        "for scenario_id, tasks in enumerate(scenarios):\n",
        "    print(f\"\\n=== Scenario {scenario_id+1} ===\")\n",
        "\n",
        "    # Initialize CILModel with default feature_dim (128) if not specified.\n",
        "    # The number of classes will be handled by update_classifier.\n",
        "    model = CILModel(input_dim).to(device)\n",
        "    buffer = ReservoirBuffer(size=4000)\n",
        "\n",
        "    # Reset datasets (important!)\n",
        "    train_dataset = IDSBaseDataset(DATA_ROOT, split=\"train\")\n",
        "    test_dataset  = IDSBaseDataset(DATA_ROOT, split=\"test\")\n",
        "\n",
        "    for task_id, seen_classes in enumerate(tasks):\n",
        "        print(f\"\\n=== Task {task_id}: {seen_classes}\")\n",
        "\n",
        "        # Update the classifier for the current number of classes for this task\n",
        "        model.update_classifier(len(seen_classes))\n",
        "        model.classifier = model.classifier.to(device)\n",
        "\n",
        "        # Re-initialize optimizer after classifier update to include new parameters\n",
        "        # and apply the correct learning rate based on task_id\n",
        "        if task_id == 0:\n",
        "            optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
        "        else:\n",
        "            # The original code used 1e-3 for task_id > 0\n",
        "            optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "        normalizer = UpToNormalizer()\n",
        "\n",
        "        # --- UP-TO NORMALIZATION STEP ---\n",
        "        task_dataset = build_task(train_dataset, seen_classes)\n",
        "        task_x = np.stack([task_dataset[i][0].numpy() for i in range(len(task_dataset))])\n",
        "\n",
        "        normalizer.update(task_x)\n",
        "\n",
        "        train_dataset.set_features(\n",
        "            normalizer.normalize(train_dataset.x)\n",
        "        )\n",
        "        test_dataset.set_features(\n",
        "            normalizer.normalize(test_dataset.x)\n",
        "        )\n",
        "\n",
        "        # train_dataset.set_features(train_dataset.x)\n",
        "        # test_dataset.set_features(test_dataset.x)\n",
        "        # --------------------------------\n",
        "\n",
        "        train_loader = DataLoader(\n",
        "            build_task(train_dataset, seen_classes),\n",
        "            batch_size=128,\n",
        "            shuffle=True\n",
        "        )\n",
        "\n",
        "        train_task(model, train_loader, buffer, optimizer)\n",
        "\n",
        "        acc, f1 = evaluate(model, test_dataset, seen_classes)\n",
        "        print(f\"Accuracy: {acc:.4f} | Macro-F1: {f1:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3272f3ae",
      "metadata": {
        "id": "3272f3ae"
      },
      "source": [
        "**Scenario 1** does not perform well the model fails to learn the new classes\n",
        "**Scenario 2** and **Scenario 3** macro f1 score gets better with more classes which means that the model is working better"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}