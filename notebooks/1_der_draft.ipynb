{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/esthy13/cil-intrusion-detection/blob/main/notebooks/1_der_draft.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NkCuth4CTgIB",
      "metadata": {
        "id": "NkCuth4CTgIB"
      },
      "source": [
        "# DER++ for Intrusion Detection (CIC-IDS)\n",
        "\n",
        "Minimal **working** implementation of **Dark Experience Replay++**\n",
        "for class-incremental intrusion detection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "383d2c74",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "383d2c74",
        "outputId": "da40937e-6581-4767-a140-8891a158a4d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'cil-intrusion-detection' already exists and is not an empty directory.\n",
            "/content/cil-intrusion-detection\n",
            "Already up to date.\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/esthy13/cil-intrusion-detection\n",
        "%cd cil-intrusion-detection\n",
        "!git pull\n",
        "# resetting the path to content to avoid issues in the rest of the notebook\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "EOVxYZQ2TgID",
      "metadata": {
        "id": "EOVxYZQ2TgID"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RDA-qUhCTgID",
      "metadata": {
        "id": "RDA-qUhCTgID"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "nGoN76koTgID",
      "metadata": {
        "id": "nGoN76koTgID"
      },
      "outputs": [],
      "source": [
        "class IDSBaseDataset(Dataset):\n",
        "    def __init__(self, root_dir, split=\"train\"):\n",
        "        \"\"\"\n",
        "        root_dir: path to 2017/\n",
        "        split: 'train' or 'test'\n",
        "        \"\"\"\n",
        "        csv_dir = os.path.join(root_dir, split)\n",
        "        csvs = glob.glob(os.path.join(csv_dir, \"*.csv\"))\n",
        "        assert len(csvs) > 0, f\"No CSV files found in {csv_dir}\"\n",
        "\n",
        "        df = pd.concat([pd.read_csv(c) for c in csvs], ignore_index=True)\n",
        "\n",
        "        labels = list(df[\"Label\"].unique())\n",
        "\n",
        "        if \"benign\" not in labels:\n",
        "            raise ValueError(\"Dataset must contain a 'benign' class\")\n",
        "\n",
        "        # Enforcing benign as class 0\n",
        "        labels = [\"benign\"] + sorted([l for l in labels if l != \"benign\"])\n",
        "\n",
        "        self.classes = labels\n",
        "        self.class_to_idx = {c: i for i, c in enumerate(self.classes)}\n",
        "\n",
        "        self.x = df.drop(columns=[\"Label\"]).values.astype(np.float32)\n",
        "        self.y = np.array(\n",
        "            [self.class_to_idx[label] for label in df[\"Label\"]],\n",
        "            dtype=np.int64\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.x[idx]), torch.tensor(self.y[idx])\n",
        "    def set_features(self, new_x):\n",
        "        assert new_x.shape == self.x.shape\n",
        "        self.x = new_x.astype(np.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-d0BVvGMTgID",
      "metadata": {
        "id": "-d0BVvGMTgID"
      },
      "source": [
        "## Task builder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "dc-LvNYbWDtQ",
      "metadata": {
        "id": "dc-LvNYbWDtQ"
      },
      "outputs": [],
      "source": [
        "class RemappedSubset(Dataset):\n",
        "    \"\"\"\n",
        "    Subset that remaps global class indices to [0..C-1]\n",
        "    \"\"\"\n",
        "    def __init__(self, dataset, indices, class_ids):\n",
        "        self.dataset = dataset\n",
        "        self.indices = indices\n",
        "        self.class_map = {cid: i for i, cid in enumerate(class_ids)}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x, y = self.dataset[self.indices[idx]]\n",
        "        return x, torch.tensor(self.class_map[y.item()])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "a58aa056",
      "metadata": {
        "id": "a58aa056"
      },
      "outputs": [],
      "source": [
        "def build_task(dataset, class_names):\n",
        "    class_ids = [dataset.class_to_idx[c] for c in class_names]\n",
        "    idxs = np.where(np.isin(dataset.y, class_ids))[0]\n",
        "    return RemappedSubset(dataset, idxs, class_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "dKlk2M2WTgID",
      "metadata": {
        "id": "dKlk2M2WTgID"
      },
      "outputs": [],
      "source": [
        "def build_scenario( all_classes, attacks_pattern, benign_class=\"benign\"):\n",
        "    \"\"\"\n",
        "    all_classes: ordered list of class names (benign must be first)\n",
        "    attacks_pattern: list of ints, number of NEW attacks per task\n",
        "                     e.g. [1,1,1] or [3,2] or [5]\n",
        "    benign_class: name of benign class (default: 'benign')\n",
        "\n",
        "    Returns:\n",
        "        tasks: list of lists of class names (cumulative)\n",
        "    \"\"\"\n",
        "\n",
        "    if benign_class not in all_classes:\n",
        "        raise ValueError(f\"Benign class '{benign_class}' not found in classes\")\n",
        "\n",
        "    if all_classes[0] != benign_class:\n",
        "        raise ValueError(\n",
        "            f\"Benign class must be index 0, got {all_classes[0]}\"\n",
        "        )\n",
        "\n",
        "    attack_classes = [c for c in all_classes if c != benign_class]\n",
        "\n",
        "    if sum(attacks_pattern) != len(attack_classes):\n",
        "        raise ValueError(\n",
        "            f\"Invalid attacks_pattern: sum={sum(attacks_pattern)}, \"\n",
        "            f\"but there are {len(attack_classes)} attack classes\"\n",
        "        )\n",
        "\n",
        "    tasks = []\n",
        "    current_index = 0\n",
        "\n",
        "    for _, n_new in enumerate(attacks_pattern):\n",
        "        current_index += n_new\n",
        "        seen_attacks = attack_classes[:current_index]\n",
        "        seen_classes = [benign_class] + seen_attacks\n",
        "        tasks.append(seen_classes)\n",
        "\n",
        "    return tasks\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class UpToNormalizer:\n",
        "    \"\"\"\n",
        "    Continual min-max normalizer using only past and present data\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.min = None\n",
        "        self.max = None\n",
        "\n",
        "    def update(self, x):\n",
        "        \"\"\"\n",
        "        x: numpy array [N, D]\n",
        "        \"\"\"\n",
        "        batch_min = x.min(axis=0)\n",
        "        batch_max = x.max(axis=0)\n",
        "\n",
        "        if self.min is None:\n",
        "            self.min = batch_min\n",
        "            self.max = batch_max\n",
        "        else:\n",
        "            self.min = np.minimum(self.min, batch_min)\n",
        "            self.max = np.maximum(self.max, batch_max)\n",
        "\n",
        "    def normalize(self, x):\n",
        "        \"\"\"\n",
        "        x: numpy array [N, D]\n",
        "        \"\"\"\n",
        "        eps = 1e-8\n",
        "        return (x - self.min) / (self.max - self.min + eps)"
      ],
      "metadata": {
        "id": "Wb3m_XX5PM08"
      },
      "id": "Wb3m_XX5PM08",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "gyLmpf46TgID",
      "metadata": {
        "id": "gyLmpf46TgID"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "MD8KacN6TgID",
      "metadata": {
        "id": "MD8KacN6TgID"
      },
      "outputs": [],
      "source": [
        "class CILModel(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes):\n",
        "        super().__init__()\n",
        "        self.fe = nn.Sequential(\n",
        "            nn.Linear(input_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 256),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.classifier = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.fe(x)\n",
        "        return self.classifier(z)\n",
        "\n",
        "    def expand_classes(self, n_new):\n",
        "        old = self.classifier\n",
        "        new = nn.Linear(old.in_features, old.out_features + n_new).to(device)\n",
        "        new.weight.data[:old.out_features] = old.weight.data\n",
        "        new.bias.data[:old.out_features] = old.bias.data\n",
        "        self.classifier = new"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fzKe9gvYTgID",
      "metadata": {
        "id": "fzKe9gvYTgID"
      },
      "source": [
        "## Replay Buffer (DER++)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "QV41fmpmTgIE",
      "metadata": {
        "id": "QV41fmpmTgIE"
      },
      "outputs": [],
      "source": [
        "class ReservoirBuffer:\n",
        "    def __init__(self, size):\n",
        "        self.size = size\n",
        "        self.indices = []\n",
        "        self.labels = []\n",
        "        self.logits = []\n",
        "        self.n_seen = 0\n",
        "\n",
        "    def add(self, indices, labels, logits):\n",
        "        \"\"\"\n",
        "        indices: list[int]\n",
        "        labels: tensor [B]\n",
        "        logits: tensor [B, C]\n",
        "        \"\"\"\n",
        "        indices = indices.tolist() if torch.is_tensor(indices) else indices\n",
        "        labels = labels.detach().cpu()\n",
        "        logits = logits.detach().cpu()\n",
        "\n",
        "        for idx, y, logit in zip(indices, labels, logits):\n",
        "            self.n_seen += 1\n",
        "\n",
        "            if len(self.indices) < self.size:\n",
        "                self.indices.append(idx)\n",
        "                self.labels.append(y)\n",
        "                self.logits.append(logit)\n",
        "            else:\n",
        "                j = random.randint(0, self.n_seen - 1)\n",
        "                if j < self.size:\n",
        "                    self.indices[j] = idx\n",
        "                    self.labels[j] = y\n",
        "                    self.logits[j] = logit\n",
        "\n",
        "    def sample(self, batch_size, current_n_classes):\n",
        "        if len(self.indices) == 0:\n",
        "            return None\n",
        "\n",
        "        idxs = np.random.choice(\n",
        "            len(self.indices),\n",
        "            min(batch_size, len(self.indices)),\n",
        "            replace=False\n",
        "        )\n",
        "\n",
        "        indices = [self.indices[i] for i in idxs]\n",
        "        labels = torch.stack([self.labels[i] for i in idxs])\n",
        "\n",
        "        padded_logits = []\n",
        "        for i in idxs:\n",
        "            logit = self.logits[i]\n",
        "\n",
        "            if logit.shape[0] < current_n_classes:\n",
        "                pad = torch.zeros(\n",
        "                    current_n_classes - logit.shape[0]\n",
        "                )\n",
        "                logit = torch.cat([logit, pad], dim=0)\n",
        "\n",
        "            padded_logits.append(logit)\n",
        "\n",
        "        logits = torch.stack(padded_logits)\n",
        "\n",
        "        return indices, labels, logits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HUKM5dMBTgIE",
      "metadata": {
        "id": "HUKM5dMBTgIE"
      },
      "source": [
        "## DER++ Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "Q0yY9tpjTgIE",
      "metadata": {
        "id": "Q0yY9tpjTgIE"
      },
      "outputs": [],
      "source": [
        "def train_task(model, loader, buffer, optimizer,\n",
        "               alpha=0.5, beta=0.5, epochs=1):\n",
        "\n",
        "    ce = nn.CrossEntropyLoss()\n",
        "    model.train()\n",
        "\n",
        "    for _ in range(epochs):\n",
        "        for batch_idx, (x, y) in enumerate(loader):\n",
        "\n",
        "            x, y = x.to(device), y.to(device)\n",
        "\n",
        "            # Forward current batch\n",
        "            out = model(x)\n",
        "            loss = ce(out, y)\n",
        "\n",
        "            # ----- DER++ Replay -----\n",
        "            buf = buffer.sample(len(x), model.classifier.out_features)\n",
        "\n",
        "            if buf is not None:\n",
        "                replay_indices, replay_labels, replay_logits = buf\n",
        "\n",
        "                # Re-fetch normalized inputs from dataset\n",
        "                bx = torch.stack([\n",
        "                    loader.dataset.dataset[i][0]\n",
        "                    for i in replay_indices\n",
        "                ]).to(device)\n",
        "\n",
        "                by = replay_labels.to(device)\n",
        "                blog = replay_logits.to(device)\n",
        "\n",
        "                replay_out = model(bx)\n",
        "\n",
        "                # Expand stored logits if classifier grew\n",
        "                if blog.shape[1] < replay_out.shape[1]:\n",
        "                    pad = torch.zeros(\n",
        "                        blog.shape[0],\n",
        "                        replay_out.shape[1] - blog.shape[1],\n",
        "                        device=device\n",
        "                    )\n",
        "                    blog = torch.cat([blog, pad], dim=1)\n",
        "\n",
        "                # DER++\n",
        "                loss += alpha * F.mse_loss(replay_out, blog)\n",
        "                loss += beta * ce(replay_out, by)\n",
        "\n",
        "            # Backprop\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # ----- Add current batch to buffer -----\n",
        "            # Recover original dataset indices\n",
        "            original_indices = [\n",
        "                loader.dataset.indices[i]\n",
        "                for i in range(\n",
        "                    batch_idx * loader.batch_size,\n",
        "                    batch_idx * loader.batch_size + len(x)\n",
        "                )\n",
        "            ]\n",
        "\n",
        "            buffer.add(original_indices, y, out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "63021c53",
      "metadata": {
        "id": "63021c53"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, dataset, seen_classes):\n",
        "    model.eval()\n",
        "\n",
        "    eval_dataset = build_task(dataset, seen_classes)\n",
        "    loader = DataLoader(eval_dataset, batch_size=256, shuffle=False)\n",
        "\n",
        "    all_preds, all_targets = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x = x.to(device)\n",
        "            preds = model(x).argmax(1).cpu().numpy()\n",
        "\n",
        "            all_preds.append(preds)\n",
        "            all_targets.append(y.numpy())\n",
        "\n",
        "    all_preds = np.concatenate(all_preds)\n",
        "    all_targets = np.concatenate(all_targets)\n",
        "\n",
        "    acc = accuracy_score(all_targets, all_preds)\n",
        "    f1  = f1_score(all_targets, all_preds, average=\"macro\")\n",
        "\n",
        "    return acc, f1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gmG72ZZQTgIE",
      "metadata": {
        "id": "gmG72ZZQTgIE"
      },
      "source": [
        "## Run Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "cc5f7872",
      "metadata": {
        "id": "cc5f7872",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8705727c-1383-4e75-aa1e-d77e4e3eda06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  cil-intrusion-detection/data/processed/2017.zip\n",
            "replace 2017/train/portscan.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ]
        }
      ],
      "source": [
        "!unzip cil-intrusion-detection/data/processed/2017.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "wU852nUOTgIE",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wU852nUOTgIE",
        "outputId": "025d94c9-203e-4d7c-b57b-0742ec1ecd6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'benign': 0, 'bot': 1, 'ddos': 2, 'dos': 3, 'ftp-patator': 4, 'portscan': 5, 'ssh-patator': 6, 'web-attack': 7}\n"
          ]
        }
      ],
      "source": [
        "# Paths\n",
        "DATA_ROOT = \"2017\"  # <-- folder created by unzip\n",
        "\n",
        "# Datasets\n",
        "train_dataset = IDSBaseDataset(DATA_ROOT, split=\"train\")\n",
        "test_dataset  = IDSBaseDataset(DATA_ROOT, split=\"test\")\n",
        "\n",
        "print(train_dataset.class_to_idx)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "f7ae59f6",
      "metadata": {
        "id": "f7ae59f6",
        "outputId": "ff1905d2-401c-4e57-ee09-057687f1c72b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Scenario 1 ===\n",
            "\n",
            "=== Task 0: ['benign', 'dos']\n",
            "Accuracy: 0.9065 | Macro-F1: 0.5438\n",
            "\n",
            "=== Task 1: ['benign', 'dos', 'ddos']\n",
            "Accuracy: 0.9149 | Macro-F1: 0.7268\n",
            "\n",
            "=== Task 2: ['benign', 'dos', 'ddos', 'portscan']\n",
            "Accuracy: 0.9158 | Macro-F1: 0.7951\n",
            "\n",
            "=== Task 3: ['benign', 'dos', 'ddos', 'portscan', 'ssh-patator']\n",
            "Accuracy: 0.9221 | Macro-F1: 0.6271\n",
            "\n",
            "=== Task 4: ['benign', 'dos', 'ddos', 'portscan', 'ssh-patator', 'ftp-patator']\n",
            "Accuracy: 0.9383 | Macro-F1: 0.5847\n",
            "\n",
            "=== Task 5: ['benign', 'dos', 'ddos', 'portscan', 'ssh-patator', 'ftp-patator', 'web-attack']\n",
            "Accuracy: 0.9455 | Macro-F1: 0.5087\n",
            "\n",
            "=== Task 6: ['benign', 'dos', 'ddos', 'portscan', 'ssh-patator', 'ftp-patator', 'web-attack', 'bot']\n",
            "Accuracy: 0.9397 | Macro-F1: 0.4377\n",
            "\n",
            "=== Scenario 2 ===\n",
            "\n",
            "=== Task 0: ['benign', 'dos', 'ddos', 'portscan', 'ssh-patator']\n",
            "Accuracy: 0.8733 | Macro-F1: 0.4353\n",
            "\n",
            "=== Task 1: ['benign', 'dos', 'ddos', 'portscan', 'ssh-patator', 'ftp-patator', 'web-attack', 'bot']\n",
            "Accuracy: 0.9127 | Macro-F1: 0.3699\n",
            "\n",
            "=== Scenario 3 ===\n",
            "\n",
            "=== Task 0: ['benign', 'dos']\n",
            "Accuracy: 0.9380 | Macro-F1: 0.7655\n",
            "\n",
            "=== Task 1: ['benign', 'dos', 'ddos', 'portscan', 'ssh-patator']\n",
            "Accuracy: 0.9275 | Macro-F1: 0.6340\n",
            "\n",
            "=== Task 2: ['benign', 'dos', 'ddos', 'portscan', 'ssh-patator', 'ftp-patator', 'web-attack', 'bot']\n",
            "Accuracy: 0.8980 | Macro-F1: 0.3674\n"
          ]
        }
      ],
      "source": [
        "input_dim = train_dataset.x.shape[1]\n",
        "\n",
        "# Task definition (example)\n",
        "all_classes = [\n",
        "    \"benign\",\n",
        "    \"dos\",\n",
        "    \"ddos\",\n",
        "    \"portscan\",\n",
        "    \"ssh-patator\",\n",
        "    \"ftp-patator\",\n",
        "    \"web-attack\",\n",
        "    \"bot\"\n",
        "]\n",
        "\n",
        "# Scenario A: 1+1+1+1+1+1+1+1\n",
        "scenario_1 = build_scenario(all_classes, [1,1,1,1,1,1,1])\n",
        "\n",
        "# Scenario B: 5+3\n",
        "scenario_2 = build_scenario(all_classes, [4, 3])\n",
        "\n",
        "# Scenario C: 2+3+3\n",
        "scenario_3 = build_scenario(all_classes, [1, 3, 3])\n",
        "\n",
        "for scenario_id, tasks in enumerate([scenario_1, scenario_2, scenario_3]):\n",
        "    print(f\"\\n=== Scenario {scenario_id+1} ===\")\n",
        "\n",
        "    model = CILModel(input_dim, len(tasks[0])).to(device)\n",
        "    buffer = ReservoirBuffer(size=2000)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "    # Reset datasets (important!)\n",
        "    train_dataset = IDSBaseDataset(DATA_ROOT, split=\"train\")\n",
        "    test_dataset  = IDSBaseDataset(DATA_ROOT, split=\"test\")\n",
        "\n",
        "    for task_id, seen_classes in enumerate(tasks):\n",
        "        normalizer = UpToNormalizer()\n",
        "        print(f\"\\n=== Task {task_id}: {seen_classes}\")\n",
        "\n",
        "        # --- UP-TO NORMALIZATION STEP ---\n",
        "        task_dataset = build_task(train_dataset, seen_classes)\n",
        "        task_x = np.stack([task_dataset[i][0].numpy() for i in range(len(task_dataset))])\n",
        "\n",
        "        normalizer.update(task_x)\n",
        "\n",
        "        train_dataset.set_features(\n",
        "            normalizer.normalize(train_dataset.x)\n",
        "        )\n",
        "        test_dataset.set_features(\n",
        "            normalizer.normalize(test_dataset.x)\n",
        "        )\n",
        "        # --------------------------------\n",
        "\n",
        "        if task_id > 0:\n",
        "            n_new = len(tasks[task_id]) - len(tasks[task_id - 1])\n",
        "            model.expand_classes(n_new)\n",
        "            optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "        train_loader = DataLoader(\n",
        "            build_task(train_dataset, seen_classes),\n",
        "            batch_size=128,\n",
        "            shuffle=True\n",
        "        )\n",
        "\n",
        "        train_task(model, train_loader, buffer, optimizer)\n",
        "\n",
        "        acc, f1 = evaluate(model, test_dataset, seen_classes)\n",
        "        print(f\"Accuracy: {acc:.4f} | Macro-F1: {f1:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3272f3ae",
      "metadata": {
        "id": "3272f3ae"
      },
      "source": [
        "**Scenario 1** does not perform well the model fails to learn the new classes\n",
        "**Scenario 2** and **Scenario 3** macro f1 score gets better with more classes which means that the model is working better"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}