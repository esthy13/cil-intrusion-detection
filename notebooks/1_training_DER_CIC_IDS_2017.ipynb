{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bf1dffb",
   "metadata": {},
   "source": [
    "# DER\n",
    "Dark Experience Replay\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911da9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/esthy13/cil-intrusion-detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bb32f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd cil-intrusion-detection\n",
    "!git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73459efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do I really need this first part??\n",
    "# import sys, os\n",
    "# sys.path.append(\"/content/cil-intrusion-detection/src\")\n",
    "\n",
    "# import here useful methods from files in src/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3118909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resetting the path to content to avoid issues in the rest of the notebook\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd83599",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO import 2017.zip dataset and unzip it from data/processed/2017.zip to content/data/processed/2017/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27177716",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class IDSBaseDataset(Dataset):\n",
    "    def __init__(self, csv_dir, class_to_idx):\n",
    "\n",
    "        \"\"\"\n",
    "        Base IDS dataset.\n",
    "        Loads all CSVs once and keeps data indexable.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        csv_paths = glob.glob(f\"{csv_dir}/*.csv\")\n",
    "        assert len(csv_paths) > 0, \"No CSV files found\"\n",
    "\n",
    "        df = pd.concat([pd.read_csv(p) for p in csv_paths], ignore_index=True)\n",
    "\n",
    "        self.class_to_idx = class_to_idx\n",
    "        self.idx_to_class = {v: k for k, v in class_to_idx.items()}\n",
    "\n",
    "        self.features = df.drop(columns=[\"Label\"]).values.astype(np.float32)\n",
    "        self.labels = np.array(\n",
    "            [self.class_to_idx[label] for label in df[\"Label\"].values],\n",
    "            dtype=np.int64\n",
    "        )\n",
    "\n",
    "        # by default: use all samples\n",
    "        self.indices = np.arange(len(self.labels))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        real_idx = self.indices[idx]\n",
    "        x = self.features[real_idx]\n",
    "        y = self.labels[real_idx]\n",
    "        return torch.tensor(x), torch.tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e28d35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2020-present, Pietro Buzzega, Matteo Boschini, Angelo Porrello, Davide Abati, Simone Calderara.\n",
    "# All rights reserved.\n",
    "# This source code is licensed under the license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "\n",
    "class BaseSampleSelection:\n",
    "    \"\"\"\n",
    "    Base class for sample selection strategies.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, buffer_size: int, device):\n",
    "        \"\"\"\n",
    "        Initialize the sample selection strategy.\n",
    "\n",
    "        Args:\n",
    "            buffer_size: the maximum buffer size\n",
    "            device: the device to store the buffer on\n",
    "        \"\"\"\n",
    "        self.buffer_size = buffer_size\n",
    "        self.device = device\n",
    "\n",
    "    def __call__(self, num_seen_examples: int) -> int:\n",
    "        \"\"\"\n",
    "        Selects the index of the sample to replace.\n",
    "\n",
    "        Args:\n",
    "            num_seen_examples: the number of seen examples\n",
    "\n",
    "        Returns:\n",
    "            the index of the sample to replace\n",
    "        \"\"\"\n",
    "\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def update(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        (optional) Update the state of the sample selection strategy.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c19e9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2020-present, Pietro Buzzega, Matteo Boschini, Angelo Porrello, Davide Abati, Simone Calderara.\n",
    "# All rights reserved.\n",
    "# This source code is licensed under the license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "\n",
    "import numpy as np\n",
    "class ReservoirSampling(BaseSampleSelection):\n",
    "    def __call__(self, num_seen_examples: int) -> int:\n",
    "        \"\"\"\n",
    "        Reservoir sampling algorithm.\n",
    "\n",
    "        Args:\n",
    "            num_seen_examples: the number of seen examples\n",
    "            buffer_size: the maximum buffer size\n",
    "\n",
    "        Returns:\n",
    "            the target index if the current image is sampled, else -1\n",
    "        \"\"\"\n",
    "        if num_seen_examples < self.buffer_size:\n",
    "            return num_seen_examples\n",
    "\n",
    "        rand = np.random.randint(0, num_seen_examples + 1)\n",
    "        if rand < self.buffer_size:\n",
    "            return rand\n",
    "        else:\n",
    "            return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57231d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "def build_task(dataset, task_class_names, buffer_size, old_task_dataset=None):\n",
    "    \"\"\"\n",
    "    dataset: IDSContinualDataset (train o test)\n",
    "    task_class_names: lista de strings, ej. ['benign', 'ddos']\n",
    "    \"\"\"\n",
    "\n",
    "    # convertir nombres a índices\n",
    "    task_class_ids = [\n",
    "        dataset.class_to_idx[c] for c in task_class_names\n",
    "    ]\n",
    "\n",
    "    #TODO what are old task labels used for?\n",
    "    # old task labels\n",
    "    if old_task_dataset is None:\n",
    "        old_task_labels = np.array([], dtype=np.int64)\n",
    "    else:\n",
    "        old_task_labels = np.unique(old_task_dataset.labels)\n",
    "\n",
    "    # seleccionar índices\n",
    "    task_indices = np.where(\n",
    "        np.isin(dataset.labels, task_class_ids)\n",
    "    )[0]\n",
    "\n",
    "    # Buffer\n",
    "\n",
    "    return Subset(dataset, task_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb62ce48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class CILModel(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        # simple MLP feature extractor\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.feature_extractor(x)\n",
    "        out = self.classifier(features)\n",
    "        return out\n",
    "\n",
    "    def increment_classifier(self, num_new_classes):\n",
    "        \"\"\"Increment the classifier to accommodate new classes.\"\"\"\n",
    "        device = next(self.parameters()).device\n",
    "\n",
    "        old_classifier = self.classifier\n",
    "        old_num_classes = old_classifier.out_features\n",
    "        new_num_classes = old_num_classes + num_new_classes\n",
    "\n",
    "        # create a new linear layer with the updated number of classes\n",
    "        new_classifier = nn.Linear(old_classifier.in_features, new_num_classes)\n",
    "\n",
    "        # copy the weights and biases from the old classifier\n",
    "        new_classifier.weight.data[:old_num_classes] = old_classifier.weight.data\n",
    "        new_classifier.bias.data[:old_num_classes] = old_classifier.bias.data\n",
    "\n",
    "        # replace classifier\n",
    "        self.classifier = new_classifier.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeac242e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is the base class for all models. It provides some useful methods and defines the interface of the models.\n",
    "\n",
    "The `observe` method is the most important one: it is called at each training iteration and it is responsible for computing the loss and updating the model's parameters.\n",
    "\n",
    "The `begin_task` and `end_task` methods are called before and after each task, respectively.\n",
    "\n",
    "The `get_parser` method returns the parser of the model. Additional model-specific hyper-parameters can be added by overriding this method.\n",
    "\n",
    "The `get_debug_iters` method returns the number of iterations to be used for debugging. Default: 3.\n",
    "\n",
    "The `get_optimizer` method returns the optimizer to be used for training. Default: SGD.\n",
    "\n",
    "The `load_buffer` method is called when a buffer is loaded. Default: do nothing.\n",
    "\n",
    "The `meta_observe`, `meta_begin_task` and `meta_end_task` methods are wrappers for `observe`, `begin_task` and `end_task` methods, respectively. They take care of updating the internal counters and of logging to wandb if installed.\n",
    "\n",
    "The `autolog_wandb` method is used to automatically log to wandb all variables starting with \"_wandb_\" or \"loss\" in the observe function. It is called by `meta_observe` if wandb is installed. It can be overridden to add custom logging.\n",
    "\"\"\"\n",
    "\n",
    "# Copyright 2020-present, Pietro Buzzega, Matteo Boschini, Angelo Porrello, Davide Abati, Simone Calderara.\n",
    "# All rights reserved.\n",
    "# This source code is licensed under the license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "\n",
    "from abc import abstractmethod\n",
    "import logging\n",
    "import sys\n",
    "from argparse import ArgumentParser, Namespace\n",
    "from contextlib import suppress\n",
    "from typing import Iterator, List, Tuple, Union, TYPE_CHECKING\n",
    "import inspect\n",
    "\n",
    "import kornia\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from datasets import get_dataset\n",
    "\n",
    "from utils.buffer import Buffer\n",
    "from utils.conf import get_device, warn_once\n",
    "from utils.kornia_utils import to_kornia_transform\n",
    "from utils.magic import persistent_locals\n",
    "from torchvision import transforms\n",
    "\n",
    "if TYPE_CHECKING:\n",
    "    from datasets.utils.continual_dataset import ContinualDataset\n",
    "    from backbone import MammothBackbone\n",
    "\n",
    "with suppress(ImportError):\n",
    "    import wandb\n",
    "\n",
    "\n",
    "class ContinualModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Continual learning model.\n",
    "    \"\"\"\n",
    "    NAME: str\n",
    "    COMPATIBILITY: List[str]\n",
    "    AVAIL_OPTIMS = ['sgd', 'adam', 'adamw']\n",
    "\n",
    "    args: Namespace  # The command line arguments\n",
    "    device: torch.device  # The device to be used for training\n",
    "    net: 'MammothBackbone'  # The backbone of the model (defined by the `dataset`)\n",
    "    loss: nn.Module  # The loss function to be used (defined by the `dataset`)\n",
    "    opt: optim.Optimizer  # The optimizer to be used for training\n",
    "    custom_scheduler: optim.lr_scheduler._LRScheduler  # (optional) The scheduler for the optimizer. If defined, it will overwrite the one defined in the `dataset`\n",
    "    # The transformation to be applied to the input data. The model will try to convert it to a kornia transform to be applicable to a batch of samples at once\n",
    "    transform: Union[transforms.Compose, kornia.augmentation.AugmentationSequential]\n",
    "    original_transform: transforms.Compose  # The original transformation to be applied to the input data. This is the one defined by the `dataset`\n",
    "    task_iteration: int  # Number of iterations in the current task\n",
    "    epoch_iteration: int  # Number of iterations in the current epoch. Updated if `epoch` is passed to observe\n",
    "    dataset: 'ContinualDataset'  # The instance of the dataset. Used to update the number of classes in the current task\n",
    "    num_classes: int  # Total number of classes in the dataset\n",
    "    n_tasks: int  # Total number of tasks in the dataset\n",
    "\n",
    "    @staticmethod\n",
    "    def get_parser(parser: ArgumentParser) -> ArgumentParser:\n",
    "        \"\"\"\n",
    "        Defines model-specific hyper-parameters, which will be added to the command line arguments. Additional model-specific hyper-parameters can be added by overriding this method.\n",
    "\n",
    "        For backward compatibility, the `parser` object may be omitted (although this should be avoided). In this case, the method should create and return a new parser.\n",
    "\n",
    "        This method may also be used to set default values for all other hyper-parameters of the framework (e.g., `lr`, `buffer_size`, etc.) with the `set_defaults` method of the parser. In this case, this method MUST update the original `parser` object and not create a new one.\n",
    "\n",
    "        Args:\n",
    "            parser: the main parser, to which the model-specific arguments will be added\n",
    "\n",
    "        Returns:\n",
    "            the parser of the model\n",
    "        \"\"\"\n",
    "        return parser\n",
    "\n",
    "    @property\n",
    "    def task_iteration(self):\n",
    "        \"\"\"\n",
    "        Returns the number of iterations in the current task.\n",
    "        \"\"\"\n",
    "        return self._task_iteration\n",
    "\n",
    "    @property\n",
    "    def epoch_iteration(self):\n",
    "        \"\"\"\n",
    "        Returns the number of iterations in the current epoch.\n",
    "        \"\"\"\n",
    "        return self._epoch_iteration\n",
    "\n",
    "    @property\n",
    "    def current_task(self):\n",
    "        \"\"\"\n",
    "        Returns the index of current task.\n",
    "        \"\"\"\n",
    "        return self._current_task\n",
    "\n",
    "    @property\n",
    "    def n_classes_current_task(self):\n",
    "        \"\"\"\n",
    "        Returns the number of classes in the current task.\n",
    "        Returns -1 if task has not been initialized yet.\n",
    "        \"\"\"\n",
    "        if hasattr(self, '_n_classes_current_task'):\n",
    "            return self._n_classes_current_task\n",
    "        else:\n",
    "            return -1\n",
    "\n",
    "    @property\n",
    "    def n_seen_classes(self):\n",
    "        \"\"\"\n",
    "        Returns the number of classes seen so far.\n",
    "        Returns -1 if task has not been initialized yet.\n",
    "        \"\"\"\n",
    "        if hasattr(self, '_n_seen_classes'):\n",
    "            return self._n_seen_classes\n",
    "        else:\n",
    "            return -1\n",
    "\n",
    "    @property\n",
    "    def n_remaining_classes(self):\n",
    "        \"\"\"\n",
    "        Returns the number of classes remaining to be seen.\n",
    "        Returns -1 if task has not been initialized yet.\n",
    "        \"\"\"\n",
    "        if hasattr(self, '_n_remaining_classes'):\n",
    "            return self._n_remaining_classes\n",
    "        else:\n",
    "            return -1\n",
    "\n",
    "    @property\n",
    "    def n_past_classes(self):\n",
    "        \"\"\"\n",
    "        Returns the number of classes seen up to the PAST task.\n",
    "        Returns -1 if task has not been initialized yet.\n",
    "        \"\"\"\n",
    "        if hasattr(self, '_n_past_classes'):\n",
    "            return self._n_past_classes\n",
    "        else:\n",
    "            return -1\n",
    "\n",
    "    @property\n",
    "    def cpt(self):\n",
    "        \"\"\"\n",
    "        Alias of `classes_per_task`: returns the raw number of classes per task.\n",
    "        Warning: return value might be either an integer or a list of integers depending on the dataset.\n",
    "        \"\"\"\n",
    "        return self._cpt\n",
    "\n",
    "    @property\n",
    "    def classes_per_task(self):\n",
    "        \"\"\"\n",
    "        Returns the raw number of classes per task.\n",
    "        Warning: return value might be either an integer or a list of integers depending on the dataset.\n",
    "        \"\"\"\n",
    "        return self._cpt\n",
    "\n",
    "    @cpt.setter\n",
    "    def cpt(self, value):\n",
    "        \"\"\"\n",
    "        Sets the number of classes per task.\n",
    "        \"\"\"\n",
    "        warn_once(\"Setting the number of classes per task is not recommended.\")\n",
    "        self._cpt = value\n",
    "\n",
    "    def __init__(self, backbone: 'MammothBackbone', loss: nn.Module,\n",
    "                 args: Namespace, transform: nn.Module, dataset: 'ContinualDataset' = None) -> None:\n",
    "        super(ContinualModel, self).__init__()\n",
    "        logging.info(\"Using {} as backbone\".format(backbone.__class__.__name__))\n",
    "        self.net = backbone\n",
    "        self.loss = loss\n",
    "        self.args = args\n",
    "        self.original_transform = transform\n",
    "        self.transform = transform\n",
    "        if dataset is None:\n",
    "            logging.error(\"No dataset provided. Will create another instance but NOTE that this **WILL** result in some bugs and possible memory leaks!\")\n",
    "            self.dataset = get_dataset(self.args)\n",
    "        else:\n",
    "            self.dataset = dataset\n",
    "        self.N_CLASSES = self.dataset.N_CLASSES\n",
    "        self.num_classes = self.N_CLASSES\n",
    "        self.N_TASKS = self.dataset.N_TASKS\n",
    "        self.n_tasks = self.N_TASKS\n",
    "        self.SETTING = self.dataset.SETTING\n",
    "        self._cpt = self.dataset.N_CLASSES_PER_TASK\n",
    "        self._current_task = 0\n",
    "\n",
    "        try:\n",
    "            if transform is not None:\n",
    "                self.transform = to_kornia_transform(transform.transforms[-1].transforms)\n",
    "                self.normalization_transform = to_kornia_transform(self.dataset.get_normalization_transform())\n",
    "            else:\n",
    "                logging.info(\"No transform provided.\")\n",
    "        except BaseException:\n",
    "            logging.error(\"could not initialize kornia transforms.\")\n",
    "            self.normalization_transform = transforms.Compose([transforms.ToPILImage(), self.dataset.TEST_TRANSFORM]) if hasattr(\n",
    "                self.dataset, 'TEST_TRANSFORM') else transforms.Compose([transforms.ToPILImage(), transforms.ToTensor(), self.dataset.get_normalization_transform()])\n",
    "\n",
    "        if self.net is not None:\n",
    "            self.opt = self.get_optimizer()\n",
    "        else:\n",
    "            logging.warning(\"no default model for this dataset. You will have to specify the optimizer yourself.\")\n",
    "            self.opt = None\n",
    "        self.device = get_device()\n",
    "\n",
    "        if not self.NAME or not self.COMPATIBILITY:\n",
    "            raise NotImplementedError('Please specify the name and the compatibility of the model.')\n",
    "\n",
    "        if self.args.label_perc != 1 and 'cssl' not in self.COMPATIBILITY:\n",
    "            logging.info('label_perc is not explicitly supported by this model -> training may break')\n",
    "\n",
    "    def to(self, device):\n",
    "        \"\"\"\n",
    "        Captures the device to be used for training.\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "        return super().to(device)\n",
    "\n",
    "    def load_buffer(self, buffer):\n",
    "        \"\"\"\n",
    "        Default way to handle load buffer.\n",
    "        \"\"\"\n",
    "\n",
    "        if isinstance(buffer, Buffer):\n",
    "            assert buffer.examples.shape[0] == self.args.buffer_size, \"Buffer size mismatch. Expected {} got {}\".format(\n",
    "                self.args.buffer_size, buffer.examples.shape[0])\n",
    "            self.buffer = buffer\n",
    "        elif isinstance(buffer, dict):  # serialized buffer\n",
    "            assert 'examples' in buffer, \"Buffer does not contain examples\"\n",
    "            assert self.buffer.buffer_size == buffer['examples'].shape[0], \"Buffer size mismatch. Expected {} got {}\".format(\n",
    "                self.buffer.buffer_size, buffer['examples'].shape[0])\n",
    "            for k, v in buffer.items():\n",
    "                setattr(self.buffer, k, v)\n",
    "            self.buffer.attributes = list(buffer.keys())\n",
    "            self.buffer.num_seen_examples = buffer['examples'].shape[0]\n",
    "        else:\n",
    "            raise ValueError(\"Buffer type not recognized\")\n",
    "\n",
    "    def get_parameters(self):\n",
    "        \"\"\"\n",
    "        Returns the parameters of the model.\n",
    "        \"\"\"\n",
    "        return self.net.parameters()\n",
    "\n",
    "    def get_optimizer(self, params: Iterator[torch.Tensor] = None, lr=None) -> optim.Optimizer:\n",
    "        \"\"\"\n",
    "        Returns the optimizer to be used for training.\n",
    "\n",
    "        Default: SGD.\n",
    "\n",
    "        Args:\n",
    "            params: the parameters to be optimized. If None, the default specified by `get_parameters` is used.\n",
    "            lr: the learning rate. If None, the default specified by the command line arguments is used.\n",
    "\n",
    "        Returns:\n",
    "            the optimizer\n",
    "        \"\"\"\n",
    "\n",
    "        params = params if params is not None else self.get_parameters()\n",
    "        if params is None:\n",
    "            logging.info(\"No parameters to optimize.\")\n",
    "            return None\n",
    "        params = list(params)\n",
    "        if len(params) == 0:\n",
    "            logging.info(\"No parameters to optimize.\")\n",
    "            return None\n",
    "\n",
    "        lr = lr if lr is not None else self.args.lr\n",
    "        # check if optimizer is in torch.optim\n",
    "        supported_optims = {optim_name.lower(): optim_name for optim_name in dir(optim) if optim_name.lower() in self.AVAIL_OPTIMS}\n",
    "        opt = None\n",
    "        if self.args.optimizer.lower() in supported_optims:\n",
    "            if self.args.optimizer.lower() == 'sgd':\n",
    "                opt = getattr(optim, supported_optims[self.args.optimizer.lower()])(params, lr=lr,\n",
    "                                                                                    weight_decay=self.args.optim_wd,\n",
    "                                                                                    momentum=self.args.optim_mom,\n",
    "                                                                                    nesterov=self.args.optim_nesterov)\n",
    "            elif self.args.optimizer.lower() == 'adam' or self.args.optimizer.lower() == 'adamw':\n",
    "                opt = getattr(optim, supported_optims[self.args.optimizer.lower()])(params, lr=lr,\n",
    "                                                                                    weight_decay=self.args.optim_wd)\n",
    "\n",
    "        if opt is None:\n",
    "            raise ValueError('Unknown optimizer: {}'.format(self.args.optimizer))\n",
    "        return opt\n",
    "\n",
    "    def get_offsets(self, task: int) -> Tuple[int, int]:\n",
    "        \"\"\"\n",
    "        Compute the start and end offset given the task.\n",
    "\n",
    "        Args:\n",
    "            task: the task index\n",
    "\n",
    "        Returns:\n",
    "            the start and end offset\n",
    "        \"\"\"\n",
    "        return self.dataset.get_offsets(task)\n",
    "\n",
    "    def get_debug_iters(self):\n",
    "        \"\"\"\n",
    "        Returns the number of iterations to be used for debugging.\n",
    "        Default: 3\n",
    "        \"\"\"\n",
    "        return 5\n",
    "\n",
    "    def begin_task(self, dataset: 'ContinualDataset') -> None:\n",
    "        \"\"\"\n",
    "        Prepares the model for the current task.\n",
    "        Executed before each task.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def end_task(self, dataset: 'ContinualDataset') -> None:\n",
    "        \"\"\"\n",
    "        Prepares the model for the next task.\n",
    "        Executed after each task.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def meta_begin_epoch(self, epoch: int, dataset: 'ContinualDataset') -> None:\n",
    "        \"\"\"\n",
    "        Wrapper for `begin_epoch` method.\n",
    "\n",
    "        Takes care of dropping updating some counters.\n",
    "        \"\"\"\n",
    "        self._epoch_iteration = 0\n",
    "        self.begin_epoch(epoch, dataset)\n",
    "\n",
    "    def meta_end_epoch(self, epoch: int, dataset: 'ContinualDataset') -> None:\n",
    "        \"\"\"\n",
    "        Wrapper for `end_epoch` method.\n",
    "        \"\"\"\n",
    "        self.end_epoch(epoch, dataset)\n",
    "\n",
    "    def begin_epoch(self, epoch: int, dataset: 'ContinualDataset') -> None:\n",
    "        \"\"\"\n",
    "        Prepares the model for the current epoch.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def end_task(self, dataset: 'ContinualDataset') -> None:\n",
    "        \"\"\"\n",
    "        Prepares the model for the next task.\n",
    "        Executed after each task.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def end_epoch(self, epoch: int, dataset: 'ContinualDataset') -> None:\n",
    "        \"\"\"\n",
    "        Prepares the model for the next epoch.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Computes a forward pass.\n",
    "\n",
    "        Args:\n",
    "            x: batch of inputs\n",
    "            task_label: some models require the task label\n",
    "\n",
    "        Returns:\n",
    "            the result of the computation\n",
    "        \"\"\"\n",
    "        return self.net(x)\n",
    "\n",
    "    def meta_observe(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Wrapper for `observe` method.\n",
    "\n",
    "        Takes care of dropping unlabeled data if not supported by the model and of logging to wandb if installed.\n",
    "\n",
    "        Args:\n",
    "            inputs: batch of inputs\n",
    "            labels: batch of labels\n",
    "            not_aug_inputs: batch of inputs without augmentation\n",
    "            kwargs: some methods could require additional parameters\n",
    "\n",
    "        Returns:\n",
    "            the value of the loss function\n",
    "        \"\"\"\n",
    "        if 'epoch' in kwargs and kwargs['epoch'] is not None:\n",
    "            epoch = kwargs['epoch']\n",
    "            if self._past_epoch != epoch:\n",
    "                self._past_epoch = epoch\n",
    "                self._epoch_iteration = 0\n",
    "\n",
    "        if 'cssl' not in self.COMPATIBILITY:  # drop unlabeled data if not supported\n",
    "            labeled_mask = args[1] != -1\n",
    "            if (~labeled_mask).any():  # if there are any unlabeled samples\n",
    "                if labeled_mask.sum() == 0:  # if all samples are unlabeled\n",
    "                    return 0\n",
    "                args = [arg[labeled_mask] if isinstance(arg, torch.Tensor) and arg.shape[0] == args[0].shape[0] else arg for arg in args]\n",
    "\n",
    "        # remove kwargs that are not needed by the observe method\n",
    "        observe_args = inspect.signature(self.observe).parameters\n",
    "        if 'kwargs' not in observe_args.keys():\n",
    "            kwargs = {k: v for k, v in kwargs.items() if k in observe_args.keys()}\n",
    "        missing_args = [\n",
    "            arg for arg, val in observe_args.items() if arg not in kwargs and\n",
    "            arg not in ['inputs', 'labels', 'not_aug_inputs'] and  # ignore args that will always be present\n",
    "            val.default is val.empty and  # avoid matching args that have default values\n",
    "            val.kind in (val.POSITIONAL_OR_KEYWORD, val.KEYWORD_ONLY)  # avoid matching **kwargs\n",
    "        ]\n",
    "        assert len(missing_args) == 0, f\"Some arguments required by observe are missing: {missing_args}. \"\n",
    "        \"Suggestion: if the missing argument is `true_labels`, you are probably missing \"\n",
    "        \"the `--noise_rate` and `--noise_type` arguments.\"\n",
    "\n",
    "        if 'wandb' in sys.modules and not self.args.nowand:\n",
    "            pl = persistent_locals(self.observe)\n",
    "            ret = pl(*args, **kwargs)\n",
    "            extra = {}\n",
    "            if isinstance(ret, dict):\n",
    "                assert 'loss' in ret, \"Loss not found in return dict\"\n",
    "                extra = {k: v for k, v in ret.items() if k != 'loss'}\n",
    "                ret = ret['loss']\n",
    "            self.autolog_wandb(pl.locals, extra=extra)\n",
    "        else:\n",
    "            ret = self.observe(*args, **kwargs)\n",
    "            if isinstance(ret, dict):\n",
    "                assert 'loss' in ret, \"Loss not found in return dict\"\n",
    "                ret = ret['loss']\n",
    "        self._task_iteration += 1\n",
    "        self._epoch_iteration += 1\n",
    "        return ret\n",
    "\n",
    "    def meta_begin_task(self, dataset):\n",
    "        \"\"\"\n",
    "        Wrapper for `begin_task` method.\n",
    "\n",
    "        Takes care of updating the internal counters.\n",
    "\n",
    "        Args:\n",
    "            dataset: the current task's dataset\n",
    "        \"\"\"\n",
    "        # update internal counters\n",
    "        self._task_iteration = 0\n",
    "        self._epoch_iteration = 0\n",
    "        self._past_epoch = 0\n",
    "        self._n_classes_current_task = self._cpt if isinstance(self._cpt, int) else self._cpt[self._current_task]\n",
    "        self._n_past_classes, self._n_seen_classes = self.get_offsets(self._current_task)\n",
    "        self._n_remaining_classes = self.N_CLASSES - self._n_seen_classes\n",
    "\n",
    "        # reload optimizer if the model has no scheduler\n",
    "        if not hasattr(self, 'scheduler') or self.custom_scheduler is None:\n",
    "            if hasattr(self, 'opt') and self.opt is not None:\n",
    "                self.opt.zero_grad(set_to_none=True)\n",
    "            self.opt = self.get_optimizer()\n",
    "        else:\n",
    "            logging.warning(\"Model defines a custom scheduler. The optimizer will not be reloaded.\")\n",
    "\n",
    "        # call the actual method\n",
    "        self.begin_task(dataset)\n",
    "\n",
    "    def meta_end_task(self, dataset):\n",
    "        \"\"\"\n",
    "        Wrapper for `end_task` method.\n",
    "\n",
    "        Takes care of updating the internal counters.\n",
    "\n",
    "        Args:\n",
    "            dataset: the current task's dataset\n",
    "        \"\"\"\n",
    "\n",
    "        self.end_task(dataset)\n",
    "        self._current_task += 1\n",
    "\n",
    "    @abstractmethod\n",
    "    def observe(self, inputs: torch.Tensor, labels: torch.Tensor,\n",
    "                not_aug_inputs: torch.Tensor, epoch: int = None) -> float:\n",
    "        \"\"\"\n",
    "        Compute a training step over a given batch of examples.\n",
    "\n",
    "        Args:\n",
    "            inputs: batch of examples\n",
    "            labels: ground-truth labels\n",
    "            kwargs: some methods could require additional parameters\n",
    "\n",
    "        Returns:\n",
    "            the value of the loss function\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def autolog_wandb(self, locals, extra=None):\n",
    "        \"\"\"\n",
    "        All variables starting with \"_wandb_\" or \"loss\" in the observe function\n",
    "        are automatically logged to wandb upon return if wandb is installed.\n",
    "        \"\"\"\n",
    "        if not self.args.nowand:\n",
    "            tmp = {k: (v.item() if isinstance(v, torch.Tensor) and v.dim() == 0 else v)\n",
    "                   for k, v in locals.items() if k.startswith('_wandb_') or 'loss' in k.lower()}\n",
    "            tmp.update(extra or {})\n",
    "            if hasattr(self, 'opt'):\n",
    "                tmp['lr'] = self.opt.param_groups[0]['lr']\n",
    "            wandb.log(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdb50f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2020-present, Pietro Buzzega, Matteo Boschini, Angelo Porrello, Davide Abati, Simone Calderara.\n",
    "# All rights reserved.\n",
    "# This source code is licensed under the license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from models.utils.continual_model import ContinualModel\n",
    "from utils.args import add_rehearsal_args, ArgumentParser\n",
    "from utils.buffer import Buffer\n",
    "\n",
    "\n",
    "class Derpp(ContinualModel):\n",
    "    \"\"\"Continual learning via Dark Experience Replay++.\"\"\"\n",
    "    NAME = 'derpp'\n",
    "    COMPATIBILITY = ['class-il', 'domain-il', 'task-il', 'general-continual']\n",
    "\n",
    "    @staticmethod\n",
    "    def get_parser(parser) -> ArgumentParser:\n",
    "        add_rehearsal_args(parser)\n",
    "        parser.add_argument('--alpha', type=float, required=True,\n",
    "                            help='Penalty weight.')\n",
    "        parser.add_argument('--beta', type=float, required=True,\n",
    "                            help='Penalty weight.')\n",
    "        return parser\n",
    "\n",
    "    def __init__(self, backbone, loss, args, transform, dataset=None):\n",
    "        super().__init__(backbone, loss, args, transform, dataset=dataset)\n",
    "\n",
    "        self.buffer = Buffer(self.args.buffer_size)\n",
    "\n",
    "    def observe(self, inputs, labels, not_aug_inputs, epoch=None):\n",
    "\n",
    "        self.opt.zero_grad()\n",
    "\n",
    "        outputs = self.net(inputs)\n",
    "\n",
    "        loss = self.loss(outputs, labels)\n",
    "\n",
    "        if not self.buffer.is_empty():\n",
    "            buf_inputs, _, buf_logits = self.buffer.get_data(self.args.minibatch_size, transform=self.transform, device=self.device)\n",
    "\n",
    "            buf_outputs = self.net(buf_inputs)\n",
    "            loss_mse = self.args.alpha * F.mse_loss(buf_outputs, buf_logits)\n",
    "            loss += loss_mse\n",
    "\n",
    "            buf_inputs, buf_labels, _ = self.buffer.get_data(self.args.minibatch_size, transform=self.transform, device=self.device)\n",
    "\n",
    "            buf_outputs = self.net(buf_inputs)\n",
    "            loss_ce = self.args.beta * self.loss(buf_outputs, buf_labels)\n",
    "            loss += loss_ce\n",
    "\n",
    "        loss.backward()\n",
    "        self.opt.step()\n",
    "\n",
    "        self.buffer.add_data(examples=not_aug_inputs,\n",
    "                             labels=labels,\n",
    "                             logits=outputs.data)\n",
    "\n",
    "        return loss.item()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
