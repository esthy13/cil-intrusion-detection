{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN1AmMQ1caJpUFb0xBJdgL2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/esthy13/cil-intrusion-detection/blob/main/notebooks/cyber_project_margarita.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import glob\n"
      ],
      "metadata": {
        "id": "7SuPRn8XoO6q"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download Original Dataset"
      ],
      "metadata": {
        "id": "W9Gwr_AJ2eNN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://205.174.165.80/CICDataset/CIC-IDS-2017/Dataset/CIC-IDS-2017/CSVs/MachineLearningCSV.zip\n",
        "!unzip MachineLearningCSV.zip"
      ],
      "metadata": {
        "id": "74wiieHyq4Wk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4aa787a3-5d19-4af3-f554-bf607c0e5009"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2026-02-04 09:12:52--  http://205.174.165.80/CICDataset/CIC-IDS-2017/Dataset/CIC-IDS-2017/CSVs/MachineLearningCSV.zip\n",
            "Connecting to 205.174.165.80:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 235102953 (224M) [application/zip]\n",
            "Saving to: ‘MachineLearningCSV.zip.1’\n",
            "\n",
            "MachineLearningCSV. 100%[===================>] 224.21M  14.9MB/s    in 28s     \n",
            "\n",
            "2026-02-04 09:13:21 (7.92 MB/s) - ‘MachineLearningCSV.zip.1’ saved [235102953/235102953]\n",
            "\n",
            "Archive:  MachineLearningCSV.zip\n",
            "replace MachineLearningCVE/Wednesday-workingHours.pcap_ISCX.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: MachineLearningCVE/Wednesday-workingHours.pcap_ISCX.csv  \n",
            "  inflating: MachineLearningCVE/Tuesday-WorkingHours.pcap_ISCX.csv  \n",
            "  inflating: MachineLearningCVE/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv  \n",
            "  inflating: MachineLearningCVE/Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv  \n",
            "  inflating: MachineLearningCVE/Monday-WorkingHours.pcap_ISCX.csv  \n",
            "  inflating: MachineLearningCVE/Friday-WorkingHours-Morning.pcap_ISCX.csv  \n",
            "  inflating: MachineLearningCVE/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv  \n",
            "  inflating: MachineLearningCVE/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob"
      ],
      "metadata": {
        "id": "sh-laeHUreko"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "csv_path = \"MachineLearningCVE\"\n",
        "csv_files = glob.glob(f\"{csv_path}/*.csv\")\n",
        "\n",
        "print(len(csv_files))\n",
        "csv_files\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XU8DCca9tEgi",
        "outputId": "b086a945-b04f-4240-d9dd-9fd3166394a8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['MachineLearningCVE/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv',\n",
              " 'MachineLearningCVE/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv',\n",
              " 'MachineLearningCVE/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv',\n",
              " 'MachineLearningCVE/Wednesday-workingHours.pcap_ISCX.csv',\n",
              " 'MachineLearningCVE/Monday-WorkingHours.pcap_ISCX.csv',\n",
              " 'MachineLearningCVE/Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv',\n",
              " 'MachineLearningCVE/Friday-WorkingHours-Morning.pcap_ISCX.csv',\n",
              " 'MachineLearningCVE/Tuesday-WorkingHours.pcap_ISCX.csv']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dfs = []\n",
        "for f in csv_files:\n",
        "    print(\"Loading:\", f)\n",
        "    dfs.append(pd.read_csv(f, encoding=\"latin1\"))\n",
        "\n",
        "df = pd.concat(dfs, ignore_index=True)\n",
        "print(df.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6C4y6CgtYaG",
        "outputId": "356212cf-aaa1-417f-a775-d05b4002bbe4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading: MachineLearningCVE/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\n",
            "Loading: MachineLearningCVE/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv\n",
            "Loading: MachineLearningCVE/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv\n",
            "Loading: MachineLearningCVE/Wednesday-workingHours.pcap_ISCX.csv\n",
            "Loading: MachineLearningCVE/Monday-WorkingHours.pcap_ISCX.csv\n",
            "Loading: MachineLearningCVE/Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv\n",
            "Loading: MachineLearningCVE/Friday-WorkingHours-Morning.pcap_ISCX.csv\n",
            "Loading: MachineLearningCVE/Tuesday-WorkingHours.pcap_ISCX.csv\n",
            "(2830743, 79)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns = df.columns.str.strip()  # quita espacios raros\n",
        "df = df.replace([np.inf, -np.inf], np.nan)\n",
        "df = df.dropna()\n"
      ],
      "metadata": {
        "id": "2eyLT2YStr1X"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isna().sum().sum()\n",
        "print(df.shape)\n",
        "df['Label'].unique()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-BSuE4nt-b7",
        "outputId": "7e99904f-643a-489a-8840-7e8be21b9290"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2827876, 79)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['BENIGN', 'DDoS', 'Web Attack ï¿½ Brute Force',\n",
              "       'Web Attack ï¿½ XSS', 'Web Attack ï¿½ Sql Injection', 'PortScan',\n",
              "       'DoS slowloris', 'DoS Slowhttptest', 'DoS Hulk', 'DoS GoldenEye',\n",
              "       'Heartbleed', 'Infiltration', 'Bot', 'FTP-Patator', 'SSH-Patator'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def map_attack(label: str) -> str:\n",
        "    label = label.strip()\n",
        "\n",
        "    patterns = [\n",
        "        (r\"^BENIGN$\", \"benign\"),\n",
        "        (r\"^PortScan$\", \"portscan\"),\n",
        "        (r\"^Bot$\", \"bot\"),\n",
        "        (r\"^Infiltration$\", \"infiltration\"),\n",
        "        (r\"^FTP-Patator$\", \"ftp-patator\"),\n",
        "        (r\"^SSH-Patator$\", \"ssh-patator\"),\n",
        "        (r\"^Heartbleed$\", \"heartbleed\"),\n",
        "        (r\"^DDoS$\", \"ddos\"),\n",
        "        (r\"Web\\s*Attack\", \"web-attack\"),\n",
        "        (r\"DoS\", \"dos\"),\n",
        "    ]\n",
        "\n",
        "    for pattern, mapped_label in patterns:\n",
        "        if re.search(pattern, label, re.IGNORECASE):\n",
        "            return mapped_label\n",
        "\n",
        "    return \"unknown\"\n"
      ],
      "metadata": {
        "id": "OqGKWY_euN43"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Label\"] = df[\"Label\"].apply(map_attack)\n",
        "print(df['Label'].unique())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VwQyspW9xFCU",
        "outputId": "122e6973-fbb8-45ad-f346-3bcba423bb8a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['benign' 'ddos' 'web-attack' 'portscan' 'dos' 'heartbleed' 'infiltration'\n",
            " 'bot' 'ftp-patator' 'ssh-patator']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Export Datasets"
      ],
      "metadata": {
        "id": "23-7mQi42nOB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "OUTPUT_DIR = \"CIL_data\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n"
      ],
      "metadata": {
        "id": "GGmfaqHR1Ru5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CLASSES = sorted(df[\"Label\"].unique())\n",
        "print(CLASSES)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tsTC5mrH1be4",
        "outputId": "c77b9b1d-084c-4e2c-ca6c-36719d21037f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['benign', 'bot', 'ddos', 'dos', 'ftp-patator', 'heartbleed', 'infiltration', 'portscan', 'ssh-patator', 'web-attack']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''for cls in CLASSES:\n",
        "    cls_df = df[df[\"Label\"] == cls]\n",
        "\n",
        "    out_path = f\"{OUTPUT_DIR}/{cls}.csv\"\n",
        "    cls_df.to_csv(out_path, index=False)\n",
        "\n",
        "    print(f\"Saved {cls}: {cls_df.shape}\")'''\n",
        "\n",
        "\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Config\n",
        "OUTPUT_DIR = \"output\"\n",
        "TRAIN_DIR = os.path.join(OUTPUT_DIR, \"train\")\n",
        "TEST_DIR = os.path.join(OUTPUT_DIR, \"test\")\n",
        "TEST_RATIO = 0.2\n",
        "SEED = 42\n",
        "\n",
        "os.makedirs(TRAIN_DIR, exist_ok=True)\n",
        "os.makedirs(TEST_DIR, exist_ok=True)\n",
        "\n",
        "CLASSES = df[\"Label\"].unique()\n",
        "\n",
        "for cls in CLASSES:\n",
        "    cls_df = df[df[\"Label\"] == cls]\n",
        "\n",
        "    train_df, test_df = train_test_split(\n",
        "        cls_df,\n",
        "        test_size=TEST_RATIO,\n",
        "        random_state=SEED,\n",
        "        shuffle=True,\n",
        "        stratify=None  # ya estamos filtrando por clase\n",
        "    )\n",
        "\n",
        "    train_path = os.path.join(TRAIN_DIR, f\"{cls}.csv\")\n",
        "    test_path = os.path.join(TEST_DIR, f\"{cls}.csv\")\n",
        "\n",
        "    train_df.to_csv(train_path, index=False)\n",
        "    test_df.to_csv(test_path, index=False)\n",
        "\n",
        "    print(\n",
        "        f\"Class {cls:15s} | \"\n",
        "        f\"Train: {train_df.shape[0]:6d} | \"\n",
        "        f\"Test: {test_df.shape[0]:6d}\"\n",
        "    )\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TjJMa7T11hl2",
        "outputId": "ecfcf048-0464-4f3a-b016-c7b288615877"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class benign          | Train: 1817056 | Test: 454264\n",
            "Class ddos            | Train: 102420 | Test:  25605\n",
            "Class web-attack      | Train:   1744 | Test:    436\n",
            "Class portscan        | Train: 127043 | Test:  31761\n",
            "Class dos             | Train: 201369 | Test:  50343\n",
            "Class heartbleed      | Train:      8 | Test:      3\n",
            "Class infiltration    | Train:     28 | Test:      8\n",
            "Class bot             | Train:   1564 | Test:    392\n",
            "Class ftp-patator     | Train:   6348 | Test:   1587\n",
            "Class ssh-patator     | Train:   4717 | Test:   1180\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Class"
      ],
      "metadata": {
        "id": "yBWbRPaE2r0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class IDSBaseDataset(Dataset):\n",
        "    def __init__(self, csv_dir, class_to_idx):\n",
        "\n",
        "        \"\"\"\n",
        "        Base IDS dataset.\n",
        "        Loads all CSVs once and keeps data indexable.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        csv_paths = glob.glob(f\"{csv_dir}/*.csv\")\n",
        "        assert len(csv_paths) > 0, \"No CSV files found\"\n",
        "\n",
        "        df = pd.concat([pd.read_csv(p) for p in csv_paths], ignore_index=True)\n",
        "\n",
        "        self.class_to_idx = class_to_idx\n",
        "        self.idx_to_class = {v: k for k, v in class_to_idx.items()}\n",
        "\n",
        "        self.features = df.drop(columns=[\"Label\"]).values.astype(np.float32)\n",
        "        self.labels = np.array(\n",
        "            [self.class_to_idx[label] for label in df[\"Label\"].values],\n",
        "            dtype=np.int64\n",
        "        )\n",
        "\n",
        "        # by default: use all samples\n",
        "        self.indices = np.arange(len(self.labels))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        real_idx = self.indices[idx]\n",
        "        x = self.features[real_idx]\n",
        "        y = self.labels[real_idx]\n",
        "        return torch.tensor(x), torch.tensor(y)"
      ],
      "metadata": {
        "id": "VpXU3WmhBj6_"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_memory_stratified(dataset, memory_size):\n",
        "    \"\"\"\n",
        "    Stratified memory sampling.\n",
        "    Each class contributes (approximately) the same number of samples.\n",
        "    \"\"\"\n",
        "\n",
        "    if memory_size <= 0:\n",
        "        return np.empty(0, dtype=int)\n",
        "\n",
        "    labels = dataset.labels\n",
        "    indices = dataset.indices\n",
        "\n",
        "    classes = np.unique(labels[indices])\n",
        "    n_classes = len(classes)\n",
        "\n",
        "    if n_classes == 0:\n",
        "        return np.empty(0, dtype=int)\n",
        "\n",
        "    base_quota = memory_size // n_classes\n",
        "    remainder = memory_size % n_classes\n",
        "\n",
        "    memory_indices = []\n",
        "\n",
        "    for c in classes:\n",
        "        class_idx = indices[labels[indices] == c]\n",
        "\n",
        "        if len(class_idx) == 0:\n",
        "            continue\n",
        "\n",
        "        k = min(base_quota, len(class_idx))\n",
        "        chosen = np.random.choice(class_idx, k, replace=False)\n",
        "        memory_indices.extend(chosen)\n",
        "\n",
        "    # distribute remaining samples (one extra per class if possible)\n",
        "    if remainder > 0:\n",
        "        for c in classes:\n",
        "            if remainder == 0:\n",
        "                break\n",
        "\n",
        "            class_idx = indices[labels[indices] == c]\n",
        "            available = np.setdiff1d(class_idx, memory_indices)\n",
        "\n",
        "            if len(available) == 0:\n",
        "                continue\n",
        "\n",
        "            extra = np.random.choice(available, 1)\n",
        "            memory_indices.append(extra[0])\n",
        "            remainder -= 1\n",
        "\n",
        "    return np.asarray(memory_indices, dtype=int)\n"
      ],
      "metadata": {
        "id": "m_2GBjbCvKcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from torch.utils.data import Subset\n",
        "\n",
        "def build_task(dataset, task_class_names, buffer_size, old_task_dataset=None):\n",
        "    \"\"\"\n",
        "    dataset: IDSContinualDataset (train o test)\n",
        "    task_class_names: lista de strings, ej. ['benign', 'ddos']\n",
        "    \"\"\"-\n",
        "\n",
        "    # convertir nombres a índices\n",
        "    task_class_ids = [\n",
        "        dataset.class_to_idx[c] for c in task_class_names\n",
        "    ]\n",
        "\n",
        "    # old task labels\n",
        "    if old_task_dataset is None:\n",
        "        old_task_labels = np.array([], dtype=np.int64)\n",
        "    else:\n",
        "        old_task_labels = np.unique(old_task_dataset.labels)\n",
        "\n",
        "    # seleccionar índices\n",
        "    task_indices = np.where(\n",
        "        np.isin(dataset.labels, task_class_ids)\n",
        "    )[0]\n",
        "\n",
        "    # Buffer\n",
        "\n",
        "    return Subset(dataset, task_indices)\n"
      ],
      "metadata": {
        "id": "HvR1PXt53tdh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "7jxfOYTmPgl_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class CILModel(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super().__init__()\n",
        "\n",
        "        # simple MLP feature extractor\n",
        "        self.feature_extractor = nn.Sequential(\n",
        "            nn.Linear(input_size, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 256),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.feature_extractor(x)\n",
        "        out = self.classifier(features)\n",
        "        return out\n",
        "\n",
        "    def increment_classifier(self, num_new_classes):\n",
        "        \"\"\"Increment the classifier to accommodate new classes.\"\"\"\n",
        "        device = next(self.parameters()).device\n",
        "\n",
        "        old_classifier = self.classifier\n",
        "        old_num_classes = old_classifier.out_features\n",
        "        new_num_classes = old_num_classes + num_new_classes\n",
        "\n",
        "        # create a new linear layer with the updated number of classes\n",
        "        new_classifier = nn.Linear(old_classifier.in_features, new_num_classes)\n",
        "\n",
        "        # copy the weights and biases from the old classifier\n",
        "        new_classifier.weight.data[:old_num_classes] = old_classifier.weight.data\n",
        "        new_classifier.bias.data[:old_num_classes] = old_classifier.bias.data\n",
        "\n",
        "        # replace classifier\n",
        "        self.classifier = new_classifier.to(device)"
      ],
      "metadata": {
        "id": "KK7gaapOPmqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ICARL"
      ],
      "metadata": {
        "id": "L4MdruAJPoJO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "from abc import ABC, abstractmethod\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "def create_optimizer(model, lr):\n",
        "    return optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "\n",
        "class BaseStrategy(ABC):\n",
        "    def __init__(self, model, criterion, batch_size, lr, new_task_lr):\n",
        "        self.model = model\n",
        "        self.criterion = criterion\n",
        "        self.batch_size = batch_size\n",
        "        self.lr = lr\n",
        "        self.new_task_lr = new_task_lr\n",
        "        self.optimizer = create_optimizer(self.model, self.lr)\n",
        "        self.seen_classes = set()\n",
        "\n",
        "    def train(self, train_loader, task_id, val_loader, epochs, patience):\n",
        "        \"\"\"\n",
        "        Main training loop with validation and early stopping.\n",
        "        \"\"\"\n",
        "        best_val_loss = float(\"inf\")\n",
        "        patience_counter = 0\n",
        "        best_model_state = copy.deepcopy(self.model.state_dict())\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            self._train_one_epoch(train_loader, task_id, epoch, epochs)\n",
        "\n",
        "            val_loss = self._validate_one_epoch(val_loader)\n",
        "            print(f\"Epoch [{epoch + 1}/{epochs}], Validation Loss: {val_loss:.4f}\")\n",
        "\n",
        "            if val_loss < best_val_loss:\n",
        "                best_val_loss = val_loss\n",
        "                patience_counter = 0\n",
        "                best_model_state = copy.deepcopy(self.model.state_dict())\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "                if patience_counter >= patience:\n",
        "                    print(f\"Early stopping at epoch {epoch + 1}\")\n",
        "                    break\n",
        "\n",
        "        self.model.load_state_dict(best_model_state)\n",
        "\n",
        "    @abstractmethod\n",
        "    def classify(self, X_eval_tensor):\n",
        "        \"\"\"\n",
        "        Classifies the given inputs using the current model.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    @abstractmethod\n",
        "    def _get_replay_loader(self, task_id):\n",
        "        \"\"\"\n",
        "        Returns the replay loader for the strategy.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    @abstractmethod\n",
        "    def _train_one_epoch(self, train_loader, task_id, epoch, epochs):\n",
        "        \"\"\"\n",
        "        Trains the model for one epoch on the provided training data.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def _validate_one_epoch(self, val_loader):\n",
        "        \"\"\"\n",
        "        Performs one epoch of validation.\n",
        "        \"\"\"\n",
        "        self.model.eval()\n",
        "        total_val_loss = 0.0\n",
        "        total_samples = 0\n",
        "        with torch.no_grad():\n",
        "            for features, labels in val_loader:\n",
        "                outputs = self.model(features)\n",
        "                loss = self.criterion(outputs, labels)\n",
        "                batch_size = features.shape[0]\n",
        "                total_val_loss += loss.item() * batch_size\n",
        "                total_samples += batch_size\n",
        "\n",
        "        return total_val_loss / total_samples\n",
        "\n",
        "    def after_task_training(\n",
        "        self, train_dataset, X_train_tensor, y_train_tensor, new_classes\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Hook called after training on a task is completed.\n",
        "        \"\"\"\n",
        "        self.model.eval()\n",
        "        self.seen_classes.update(new_classes)\n",
        "\n",
        "    def after_task_model_update(self, **kwargs):\n",
        "        \"\"\"\n",
        "        Hook called after the model is updated for a new task.\n",
        "        \"\"\"\n",
        "        self.optimizer = create_optimizer(self.model, self.new_task_lr)"
      ],
      "metadata": {
        "id": "L8n7EWFRPoja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import cycle\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "\n",
        "class iCaRL(BaseStrategy):\n",
        "    def __init__(\n",
        "        self,\n",
        "        model,\n",
        "        criterion,\n",
        "        batch_size,\n",
        "        lr,\n",
        "        new_task_lr,\n",
        "        buffer_size,\n",
        "        device,\n",
        "        distillation_lambda,\n",
        "    ):\n",
        "        super().__init__(model, criterion, batch_size, lr, new_task_lr)\n",
        "        self.buffer_size = buffer_size\n",
        "        self.device = device\n",
        "        self.distillation_lambda = distillation_lambda\n",
        "\n",
        "        self.replay_buffer = []\n",
        "        self.exemplars = {}\n",
        "        self.exemplar_means = {}\n",
        "\n",
        "    def _get_replay_loader(self, task_id):\n",
        "        if task_id == 0 or not self.replay_buffer:\n",
        "            return None\n",
        "\n",
        "        # stack the data and labels from the memory buffer into tensors\n",
        "        replay_features = torch.stack([item[0] for item in self.replay_buffer])\n",
        "        replay_labels = torch.stack([item[1] for item in self.replay_buffer])\n",
        "        replay_logits = torch.stack([item[2] for item in self.replay_buffer])\n",
        "\n",
        "        replay_dataset = TensorDataset(replay_features, replay_labels, replay_logits)\n",
        "\n",
        "        return DataLoader(\n",
        "            dataset=replay_dataset, batch_size=self.batch_size, shuffle=True\n",
        "        )\n",
        "\n",
        "    def _train_one_epoch(self, train_loader, task_id, epoch, epochs):\n",
        "        # parallel iteration over train and replay loaders\n",
        "        train_iter = iter(train_loader)\n",
        "\n",
        "        replay_loader = self._get_replay_loader(task_id)\n",
        "        if replay_loader is not None:\n",
        "            replay_iter = cycle(replay_loader)\n",
        "        else:\n",
        "            replay_iter = None\n",
        "\n",
        "        # loop until the training data is exhausted\n",
        "        for i in range(len(train_loader)):\n",
        "            self.optimizer.zero_grad()\n",
        "            loss = 0.0\n",
        "\n",
        "            # training on new data\n",
        "            try:\n",
        "                features, labels = next(train_iter)\n",
        "                outputs = self.model(features)\n",
        "                loss += self.criterion(outputs, labels)\n",
        "            except StopIteration:\n",
        "                pass\n",
        "\n",
        "            if replay_iter is not None:\n",
        "                # training on replay data with distillation\n",
        "                try:\n",
        "                    replay_features, replay_labels, teacher_logits = next(replay_iter)\n",
        "\n",
        "                    student_logits = self.model(replay_features)\n",
        "                    loss += self.criterion(student_logits, replay_labels)\n",
        "\n",
        "                    # distillation loss should only be calculated on the logits of old classes\n",
        "                    loss += self.distillation_lambda * self._distillation_loss(\n",
        "                        student_logits, teacher_logits\n",
        "                    )\n",
        "\n",
        "                except StopIteration:\n",
        "                    pass\n",
        "\n",
        "            if loss != 0.0:\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "            if (i + 1) % 200 == 0:\n",
        "                print(\n",
        "                    f\"Task [{task_id + 1}], Epoch [{epoch + 1}/{epochs}], \"\n",
        "                    f\"Step [{i + 1}/{len(train_loader)}], Loss: {loss.item():.4f}\"\n",
        "                )\n",
        "\n",
        "    def classify(self, X_eval_tensor):\n",
        "        preds = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            eval_features = self.model.feature_extractor(X_eval_tensor)\n",
        "            for feature in eval_features:\n",
        "                closest_class = None\n",
        "                closest_distance = float(\"inf\")\n",
        "\n",
        "                for c in self.exemplar_means:\n",
        "                    # find the closest class mean\n",
        "                    dist = torch.norm(feature - self.exemplar_means[c])\n",
        "                    if dist < closest_distance:\n",
        "                        closest_distance = dist\n",
        "                        closest_class = c\n",
        "                preds.append(closest_class)\n",
        "\n",
        "        return torch.tensor(preds).to(self.device)\n",
        "\n",
        "    def after_task_training(\n",
        "        self, train_dataset, X_train_tensor, y_train_tensor, new_classes\n",
        "    ):\n",
        "        super().after_task_training(\n",
        "            train_dataset, X_train_tensor, y_train_tensor, new_classes\n",
        "        )\n",
        "        self._update_memory(X_train_tensor, y_train_tensor, new_classes)\n",
        "        self._compute_class_means()\n",
        "\n",
        "    def _update_memory(self, X_train_tensor, y_train_tensor, new_classes):\n",
        "        num_total_classes = len(self.seen_classes)\n",
        "        exemplars_per_class = self.buffer_size // num_total_classes\n",
        "\n",
        "        # trim exemplars for old classes if necessary\n",
        "        for c in self.exemplars:\n",
        "            if c not in new_classes:\n",
        "                self.exemplars[c] = self.exemplars[c][:exemplars_per_class]\n",
        "\n",
        "        self.model.eval()\n",
        "\n",
        "        for c in new_classes:\n",
        "            indices = (y_train_tensor == c).nonzero(as_tuple=True)[0]\n",
        "            current_features = X_train_tensor[indices]\n",
        "            current_labels = y_train_tensor[indices]\n",
        "\n",
        "            # if we have old exemplars for this class, combine them with the new data\n",
        "            if c in self.exemplars and len(self.exemplars[c]) > 0:\n",
        "                old_features = torch.stack([item[0] for item in self.exemplars[c]])\n",
        "                old_labels = torch.stack([item[1] for item in self.exemplars[c]])\n",
        "\n",
        "                candidate_features = torch.cat((old_features, current_features), dim=0)\n",
        "                candidate_labels = torch.cat((old_labels, current_labels), dim=0)\n",
        "            else:\n",
        "                candidate_features = current_features\n",
        "                candidate_labels = current_labels\n",
        "\n",
        "            if len(candidate_features) == 0:\n",
        "                continue\n",
        "\n",
        "            with torch.no_grad():\n",
        "                feature = self.model.feature_extractor(candidate_features)\n",
        "\n",
        "            # normalize features\n",
        "            feature = torch.nn.functional.normalize(feature, p=2, dim=1)\n",
        "\n",
        "            # compute class mean\n",
        "            class_mean = torch.mean(feature, dim=0)\n",
        "\n",
        "            # herding\n",
        "            selected_indices = []\n",
        "            current_sum = torch.zeros_like(class_mean)\n",
        "\n",
        "            current_k = min(exemplars_per_class, len(feature))\n",
        "\n",
        "            for k in range(current_k):\n",
        "                # we want to select the exemplar that, when added to the current sum,\n",
        "                # brings the average closest to the class mean\n",
        "                target = class_mean * (k + 1) - current_sum\n",
        "\n",
        "                distances = torch.norm(feature - target, dim=1)\n",
        "\n",
        "                if len(selected_indices) > 0:\n",
        "                    # avoid selecting the same exemplar again\n",
        "                    distances[selected_indices] = float(\"inf\")\n",
        "\n",
        "                # select the best exemplar\n",
        "                best_idx = int(torch.argmin(distances).item())\n",
        "\n",
        "                selected_indices.append(best_idx)\n",
        "                current_sum += feature[best_idx]\n",
        "\n",
        "            self.exemplars[c] = [\n",
        "                (candidate_features[idx], candidate_labels[idx])\n",
        "                for idx in selected_indices\n",
        "            ]\n",
        "\n",
        "        # update replay buffer\n",
        "        self.replay_buffer = []\n",
        "        with torch.no_grad():\n",
        "            for c in self.exemplars:\n",
        "                for feature, label in self.exemplars[c]:\n",
        "                    logits = self.model(feature.unsqueeze(0)).squeeze(0)\n",
        "                    self.replay_buffer.append((feature, label, logits))\n",
        "\n",
        "    def _distillation_loss(self, student_logits, teacher_logits, T=2.0):\n",
        "        num_teacher_cls = teacher_logits.shape[1]\n",
        "\n",
        "        student_dist = F.log_softmax(student_logits[:, :num_teacher_cls] / T, dim=1)\n",
        "        teacher_dist = F.softmax(teacher_logits / T, dim=1)\n",
        "\n",
        "        return nn.KLDivLoss(reduction=\"batchmean\")(student_dist, teacher_dist) * (T**2)\n",
        "\n",
        "    def _compute_class_means(self):\n",
        "        self.model.eval()\n",
        "\n",
        "        for c in self.exemplars:\n",
        "            class_features = []\n",
        "            # extract and normalize features\n",
        "            for feature, _ in self.exemplars[c]:\n",
        "                with torch.no_grad():\n",
        "                    feat = self.model.feature_extractor(feature.unsqueeze(0))\n",
        "                    feat = torch.nn.functional.normalize(feat, p=2, dim=1)\n",
        "                    class_features.append(feat)\n",
        "\n",
        "            # compute mean of class features\n",
        "            class_features = torch.cat(class_features, dim=0)\n",
        "            class_mean = torch.mean(class_features, dim=0)\n",
        "            class_mean = torch.nn.functional.normalize(class_mean, p=2, dim=0)\n",
        "\n",
        "            self.exemplar_means[c] = class_mean"
      ],
      "metadata": {
        "id": "7XZ0wRJ3YiTy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}